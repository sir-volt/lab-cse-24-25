# 105 Spark Streaming

The goal of this lab is to acquire familiarity with streaming data analysis in Spark.

- [Spark Streaming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html)

## 305-0 Setup

For the streaming scenario, we need a streaming source. 
It can be done in two ways:

- A Netcat server instantiated on the cluster that sends some data
- A Kafka cluster to collect and distribute messages

For simplicity, we rely on the first option.

To produce messages through a Netcat server:

- Open MobaXTerm
- Run a Netcat server with ```nc -lk <port>```, where ```<port>``` is an available port number (e.g., 9999)
- Now, anything you write (or copy/paste) will be sent to any client connecting to the Netcat server

### Deploy streaming application via LOCAL spark-submit

Deploying applications via spark-submit is the best way to run streaming applications (rather than notebooks and spark-shells).

- Open the Spark project with IntelliJ
- Run `./gradlew` on the terminal to build the jar
- Open a GitBash and use ```spark-submit --class Streaming BigDataLab.jar <exerciseNumber> <host> <port>```
    - ```<host>``` is the address of the Master node
    - ```<port>``` is the same entered above
- When done, terminate the streaming application with Ctrl+C
  
Some exercises require additional parameters; in such cases, proper usage is indicated in the exercises' description.

#### Troubleshooting

*The connection dropped when the Netcat server was running;
now, when you reconnect, the port is occupied.* 
Find running processes with ```ps aux | grep "nc -lk"``` 
and kill them with ```kill <pid>```.


## 305-1 Testing the streaming application

Copy/Paste some content (e.g., the text from the ```divinacommedia.txt```
file) into the Netcat server and see what happens on the application's console.

## 305-2 Word count

Same as above.

## 305-3 Enabling checkpoint

Checkpoints allow the application to restart from where it last stopped.
               
NOTICE: you need to create a local directory to store the checkpoint data.
For instance, ```c:/spark-3.5.1-bin-hadoop3/streaming/ex3```.

Then run the application; you need to provide 
an additional ```<path>``` parameter to the ```spark-submit``` command
to indicate the local path, e.g., ```file:///c:/spark-3.5.1-bin-hadoop3/streaming/ex3```.

## 305-4 Enabling State

State allows the job to continuously update a temporary result (i.e., the state).
   
NOTICE: you need to either
- create a DIFFERENT directory to store the checkpoint data
- empty the previous directory

Otherwise, the application will re-run the job already checkpointed in the directory!

The additional ```<path>``` parameter is required.

## 305-5 Sliding windows

This job carries out word counting on a sliding window 
that is wide 30 seconds and is updated every 3 seconds.

## 305-6 Trending hashtags

The dataset for this exercise (and for the next ones) 
is the content of ```dataset/tweet.dsv```, 
which contains a set of public tweets that discussed the topic
of vaccines back in 2016.

- Copy/Paste some content from the ```dataset/tweet.dsv``` file.
 
This job is a simple evolution of word counting to carry out hashtag 
counting via sliding windows. 
The window is wide 1 minute and it is updated every 5 seconds.

## 305-7 Incremental count of tweets by city

This is a stateful job to incrementally count the number of tweets by city. 

The additional ```<path>``` parameter is required. Remember to either create a new directory or to empty the previous one.

## 305-8 Incremental count of tweets by city

This job extends the previous one by calculating also the average sentiment 
(per country instead of per city).

The additional ```<path>``` parameter is required. Remember to either create a new directory or to empty the previous one.