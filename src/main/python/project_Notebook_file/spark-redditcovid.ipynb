{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-05T16:06:33.433643Z",
     "start_time": "2025-02-05T16:06:33.172779Z"
    }
   },
   "source": "import org.apache.spark",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T16:06:36.233365Z",
     "start_time": "2025-02-05T16:06:35.907017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val path_to_datasets = \"../../../../datasets/big/\"\n",
    "\n",
    "val path_ml_posts = path_to_datasets + \"the-reddit-covid-dataset-posts.csv\"\n",
    "val path_ml_comments = path_to_datasets + \"the-reddit-covid-dataset-comments.csv\""
   ],
   "id": "e3447d001ccfc3b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_to_datasets: String = ../../../../datasets/big/\r\n",
       "path_ml_posts: String = ../../../../datasets/big/the-reddit-covid-dataset-posts.csv\r\n",
       "path_ml_comments: String = ../../../../datasets/big/the-reddit-covid-dataset-comments.csv\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f82e1326f40e4ea5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T16:06:40.768349Z",
     "start_time": "2025-02-05T16:06:40.047784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.util.Calendar\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "object CovidConversationsParser {\n",
    "\n",
    "    val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "    val pipeRegex = \"\\\\|(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "    val quotes = \"\\\"\"\n",
    "\n",
    "    /** Convert from timestamp (String) to year (Int) */\n",
    "    def yearFromTimestamp(timestamp: String): Int = {\n",
    "        val cal = Calendar.getInstance()\n",
    "        cal.setTimeInMillis(timestamp.trim.toLong * 1000L)\n",
    "        cal.get(Calendar.YEAR)\n",
    "    }\n",
    "    \n",
    "    /** Combines splits created into their respective string values */\n",
    "    def combineText(input: Array[String], initialSplit: Int): (String, Int) = {\n",
    "        if (initialSplit >= input.length) return (\"\", initialSplit)\n",
    "    \n",
    "        var fullText = input(initialSplit)\n",
    "        var nSplit = initialSplit + 1\n",
    "    \n",
    "        if (fullText.startsWith(quotes)) {\n",
    "            while (nSplit < input.length && !input(nSplit).trim.endsWith(quotes)) {\n",
    "                fullText += input(nSplit).trim\n",
    "                nSplit += 1\n",
    "            }\n",
    "            if (nSplit < input.length) {\n",
    "                fullText += input(nSplit).trim\n",
    "                nSplit += 1\n",
    "            }\n",
    "        }\n",
    "        (fullText, nSplit)\n",
    "    }\n",
    "\n",
    "    /** Function to parse reddit posts\n",
    "    *\n",
    "    *  @param line line that has to be parsed\n",
    "    *  @return tuple containing id,subreddit.id,subreddit.name,subreddit.nsfw,created_utc,permalink,domain,url,selftext,title,score. none in case of input errors\n",
    "    */\n",
    "    def parseRedditPost(line: String): Option[(String, String, String, Boolean, Int, String, String, java.io.Serializable, java.io.Serializable, String, String)] = {\n",
    "        try {\n",
    "            val input = line.split(commaRegex)\n",
    "            val url = if(input(8).trim.nonEmpty) input(8).trim else None\n",
    "            val selftext = if(input(9).trim.nonEmpty) combineText(input, 9)._1 else None\n",
    "            var n = combineText(input, 9)._2\n",
    "            val title = combineText(input, n)._1\n",
    "            n = combineText(input, n)._2\n",
    "            val score = combineText(input, n)._1\n",
    "            Some((input(1).trim, input(2).trim, input(3).trim, input(4).trim.toBoolean, yearFromTimestamp(input(5)), input(6).trim, input(7).trim, url, selftext, title, score))   \n",
    "        } catch {\n",
    "            case _: Exception => None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /** Function to parse reddit comments\n",
    "    *\n",
    "    *  @param line line that has to be parsed\n",
    "    *  @return tuple containing id,subreddit.id,subreddit.name,subreddit.nsfw,created_utc,permalink,body,sentiment,score. none in case of input errors\n",
    "    */\n",
    "    def parseRedditComment(line: String): Option[(String, String, String, Boolean, Int, String, String, Double, Int)] = {\n",
    "        try {\n",
    "            val input = line.split(commaRegex)\n",
    "            val body = combineText(input, 7)._1\n",
    "            var n = combineText(input, 7)._2\n",
    "            val sentiment = combineText(input, n)._1.toDouble\n",
    "            n = combineText(input, n)._2\n",
    "            val score = combineText(input, n)._1.toInt\n",
    "            Some((input(1).trim, input(2).trim, input(3).trim, input(4).trim.toBoolean, yearFromTimestamp(input(5)), input(6).trim, body, sentiment, score))\n",
    "        } catch {\n",
    "            case _: Exception => None\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "id": "c887d7e89a51786e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.Calendar\r\n",
       "import org.apache.spark.sql.SaveMode\r\n",
       "import org.apache.spark.HashPartitioner\r\n",
       "defined object CovidConversationsParser\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:58:21.587212Z",
     "start_time": "2025-02-05T15:58:21.340607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "//case class CovidPost(id:String, subredditId:String, subredditName:String, subredditNSFW:boolean, created_UTC: Calendar,permalink:String, domain:String, URL:String, selftext:Option[String], title:String, score:Int)\n",
    "\n",
    "//case class CovidComment(id:String, subredditId:String, subredditName:String, subredditNSFW:boolean, created_UTC: Calendar,permalink:String, body:String, sentiment:Float, score:Int)"
   ],
   "id": "9b5e0aa76d4631f8",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "517000d0f73409c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T16:16:04.602678Z",
     "start_time": "2025-02-05T16:16:03.708948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "//per recuperare dei samples dei files\n",
    "//sc.textFile(path_ml_posts).sample(false, 0.2).coalesce(1).toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(\"../../../../datasets/\")"
   ],
   "id": "6ef50ca52909c10e",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T16:52:32.602833Z",
     "start_time": "2025-02-05T16:52:32.030176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "/*getting RDDs of Comments and Posts*/\n",
    "val rddComments = sc.textFile(path_ml_posts).flatMap(CovidConversationsParser.parseRedditComment)\n",
    "val rddPosts = sc.textFile(path_ml_comments).flatMap(CovidConversationsParser.parseRedditPost)\n",
    "rddPosts.collect()"
   ],
   "id": "72a3df9388606c92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddComments: org.apache.spark.rdd.RDD[(String, String, String, Boolean, Int, String, String, Double, Int)] = MapPartitionsRDD[74] at flatMap at <console>:52\r\n",
       "rddPosts: org.apache.spark.rdd.RDD[(String, String, String, Boolean, Int, String, String, java.io.Serializable, java.io.Serializable, String, String)] = MapPartitionsRDD[77] at flatMap at <console>:53\r\n",
       "res9: Array[(String, String, String, Boolean, Int, String, String, java.io.Serializable, java.io.Serializable, String, String)] = Array()\r\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T18:04:49.329100Z",
     "start_time": "2025-02-05T18:04:49.012765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "//PART 1: Aggregate on temporal dimension and obtain percentage of posts classified as NSFW\n",
    "//Posts are :(id,subreddit.id,subreddit.name,subreddit.nsfw,created_utc,permalink,domain,url,selftext,title,score)\n",
    "val percentageNSFWPosts = rddPosts.map(x => (x._5, if (x._4) 1 else 0))  // (created_utc, nsfw flag)\n",
    ".reduceByKey(_ + _)  // count NSFW posts per timestamp\n",
    ".mapValues(nsfwCount => {\n",
    "    val totalPostsAtTime = rddPosts.filter(y => y._5 == y._5).count()\n",
    "    (nsfwCount.toDouble / totalPostsAtTime) * 100\n",
    "})\n",
    "\n",
    "//.coalesce(1) //riduzione a singola partizione (per analisi, con singola partizione abbiamo singolo file da analizzare)\n",
    "//.collect()"
   ],
   "id": "ef8bd011160741f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "percentageNSFWPosts: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[87] at mapValues at <console>:51\r\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T18:04:31.725002Z",
     "start_time": "2025-02-05T18:04:31.307648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "//PART 2: Aggregate on temporal dimension and obtain average sentiment in comments and percentage of comments classified as NSFW\n",
    "//Comments are: (id,subreddit.id,subreddit.name,subreddit.nsfw,created_utc,permalink,body,sentiment,score)\n",
    "val avgSentimentWithNSFWPost = rddComments.map(x => (x._5, (if (x._4) 1 else 0, x._8)))  // (created_utc, (nsfw flag, sentiment))\n",
    ".reduceByKey((a, b) => (a._1 + b._1, a._2 + b._2))  // aggregate NSFW count and sentiment\n",
    ".mapValues(avgSentimentAndNSFWCount => {\n",
    "    val totalCommentsAtTime = rddComments.filter(y => y._5 == y._5).count()\n",
    "    val nsfwPercentage = (avgSentimentAndNSFWCount._1.toDouble / totalCommentsAtTime) * 100\n",
    "    val avgSentiment = avgSentimentAndNSFWCount._2 / totalCommentsAtTime\n",
    "    (nsfwPercentage, avgSentiment)\n",
    "})"
   ],
   "id": "32eee082e5a2a05a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgSentimentWithNSFWPost: org.apache.spark.rdd.RDD[(Int, (Double, Double))] = MapPartitionsRDD[84] at mapValues at <console>:51\r\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T18:08:59.823425Z",
     "start_time": "2025-02-05T18:08:59.565234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// PART 3: Join on Temporal Dimension\n",
    "val finalResult = percentageNSFWPosts.join(avgSentimentWithNSFWPost)"
   ],
   "id": "afc6dfa8abb55193",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalResult: org.apache.spark.rdd.RDD[(Int, (Double, (Double, Double)))] = MapPartitionsRDD[90] at join at <console>:49\r\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
