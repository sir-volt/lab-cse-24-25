{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-13T12:07:52.183297Z",
     "start_time": "2025-02-13T12:07:22.844021Z"
    }
   },
   "source": "import org.apache.spark",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.226.1:4040\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1739448461710)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:09:10.614502Z",
     "start_time": "2025-02-13T12:09:09.718125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val path_to_bigDatasets = \"../../../../datasets/big/\"\n",
    "val path_to_datasets = \"../../../../datasets/\"\n",
    "\n",
    "val path_Fullml_posts = path_to_bigDatasets + \"the-reddit-covid-dataset-posts.csv\"\n",
    "val path_Fullml_comments = path_to_bigDatasets + \"the-reddit-covid-dataset-comments.csv\"\n",
    "\n",
    "val path_sample_posts = path_to_datasets + \"postsSample.csv\"\n",
    "val path_sample_comments = path_to_datasets + \"commentsSample.csv\""
   ],
   "id": "e3447d001ccfc3b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_to_bigDatasets: String = ../../../../datasets/big/\r\n",
       "path_to_datasets: String = ../../../../datasets/\r\n",
       "path_Fullml_posts: String = ../../../../datasets/big/the-reddit-covid-dataset-posts.csv\r\n",
       "path_Fullml_comments: String = ../../../../datasets/big/the-reddit-covid-dataset-comments.csv\r\n",
       "path_sample_posts: String = ../../../../datasets/postsSample.csv\r\n",
       "path_sample_comments: String = ../../../../datasets/commentsSample.csv\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:09:14.647153Z",
     "start_time": "2025-02-13T12:09:13.336556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.util.Calendar\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "object CovidConversationsParser {\n",
    "\n",
    "    val commaSplit = \",\"\n",
    "    val quotes = \"\\\"\"\n",
    "\n",
    "    /** Convert from timestamp (String) to day (Int) */\n",
    "    def dayFromTimestamp(timestamp: String): Int = {\n",
    "        val cal = Calendar.getInstance()\n",
    "        cal.setTimeInMillis(timestamp.trim.toLong * 1000L)\n",
    "        cal.get(Calendar.DAY_OF_MONTH)\n",
    "    }\n",
    "\n",
    "    /** Function to parse reddit posts\n",
    "    *\n",
    "    *  @param line line that has to be parsed\n",
    "    *  @return tuple containing id,subreddit.id,subreddit.name,subreddit.nsfw,created_utc,permalink,domain,url,selftext,title,score. none in case of input errors\n",
    "    */\n",
    "    def parseRedditPost(line: String): Option[(String, String, String, Boolean, Int, String, String, String, String, String, Int)] = {\n",
    "        try {\n",
    "            val input = line.split(commaSplit)\n",
    "            if(input.size != 12) {\n",
    "                return Some((\"\", \"\", \"\", false, -1, \"\", \"\", \"\", \"\", \"\", -1))\n",
    "            }\n",
    "            var url = \"None\"\n",
    "            if(input(8).trim.nonEmpty) {\n",
    "                url = input(8).trim\n",
    "            }\n",
    "            var selftext = \"None\"\n",
    "            if(input(9).trim.nonEmpty) {\n",
    "                selftext = input(9).trim\n",
    "            }\n",
    "            val number = input(11).trim.replaceAll(quotes, \"\")\n",
    "            Some((input(1).trim, input(2).trim, input(3).trim, input(4).trim.toBoolean, dayFromTimestamp(input(5)), input(6).trim, input(7).trim, url, selftext, input(10).trim , number.toInt))\n",
    "        } catch {\n",
    "            case _: Exception => None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /** Function to parse reddit comments\n",
    "    *\n",
    "    *  @param line line that has to be parsed\n",
    "    *  @return tuple containing id,subreddit.id,subreddit.name,subreddit.nsfw,created_utc,permalink,body,sentiment,score. none in case of input errors\n",
    "    */\n",
    "    def parseRedditComment(line: String): Option[(String, String, String, Boolean, Int, String, String, Double, Int)] = {\n",
    "        try {\n",
    "            val input = line.split(commaSplit)\n",
    "            if(input.size != 10) {\n",
    "                return Some((\"\", \"\", \"\", false, -1, \"\", \"\", -0.1, -1))\n",
    "            }\n",
    "            val number = input(9).trim.replaceAll(quotes, \"\")\n",
    "            Some((input(1).trim, input(2).trim, input(3).trim, input(4).trim.toBoolean, dayFromTimestamp(input(5)), input(6).trim, input(7).trim, input(8).trim.toDouble, number.toInt))\n",
    "        } catch {\n",
    "            case _: Exception => None\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "id": "c887d7e89a51786e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.Calendar\r\n",
       "import org.apache.spark.sql.SaveMode\r\n",
       "import org.apache.spark.HashPartitioner\r\n",
       "defined object CovidConversationsParser\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "//per recuperare dei samples dei files\n",
    "//sc.textFile(path_Fullml_posts).sample(false, 0.05).coalesce(1).toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(\"../../../../datasets/sample\")\n",
    "sc.textFile(path_Fullml_comments).sample(false, 0.02).coalesce(1).toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(\"../../../../datasets/sample\")"
   ],
   "id": "6ef50ca52909c10e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:09:21.809414Z",
     "start_time": "2025-02-13T12:09:18.823600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "/*getting RDDs of Comments and Posts*/\n",
    "val rddPosts = sc.textFile(path_sample_posts).flatMap(CovidConversationsParser.parseRedditPost).filter(x => x != (\"\",\"\",\"\",false,-1,\"\",\"\",\"\",\"\",\"\",-1))\n",
    "val rddComments = sc.textFile(path_sample_comments).flatMap(CovidConversationsParser.parseRedditComment)\n",
    ".filter(x => x != (\"\", \"\", \"\", false, -1, \"\", \"\", -0.1, -1))\n",
    "rddComments.first()"
   ],
   "id": "72a3df9388606c92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddPosts: org.apache.spark.rdd.RDD[(String, String, String, Boolean, Int, String, String, String, String, String, Int)] = MapPartitionsRDD[3] at filter at <console>:32\r\n",
       "rddComments: org.apache.spark.rdd.RDD[(String, String, String, Boolean, Int, String, String, Double, Int)] = MapPartitionsRDD[7] at filter at <console>:34\r\n",
       "res0: (String, String, String, Boolean, Int, String, String, Double, Int) = (hi1vjyo,2y77d,antiwork,false,26,https://old.reddit.com/r/antiwork/comments/qft1w1/i_work_for_a_lab_doing_covid_testing_everyday_ive/hi1vjyo/,Can I come love with you? I can do Covid testing for free and take up very little space.,0.8176,11)\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:09:27.690276Z",
     "start_time": "2025-02-13T12:09:26.527334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "//PART 1: Aggregate on temporal dimension and obtain percentage of posts classified as NSFW\n",
    "//Posts are :(id,subreddit.id,subreddit.name,subreddit.nsfw,created_utc,permalink,domain,url,selftext,title,score)\n",
    "/* solution is better using  reduceByKey becasue:\n",
    "    -Avoid collecting all values into memory before processing\n",
    "    -Perform aggregation as data is being processed\n",
    "    -Reduce network shuffle by combining data locally first\n",
    "*/\n",
    "val percentageNSFWPosts = rddPosts.map(x => (x._5, (1, if (x._4) 1 else 0))) // (created_utc, (posts count, nsfw flag))\n",
    ".reduceByKey ({ case ((total1, nsfw1), (total2, nsfw2)) =>\n",
    "(total1 + total2, nsfw1 + nsfw2)\n",
    "})\n",
    ".mapValues ({ case (total, nsfw) =>\n",
    "    //Calculate the percentage\n",
    "    val percentage = (nsfw.toDouble * 100) / total\n",
    "    \"Percentage NSFW Posts: \" + percentage + \"%\"\n",
    "})\n",
    "\n",
    "//.coalesce(1) //riduzione a singola partizione (per analisi, con singola partizione abbiamo singolo file da analizzare)\n",
    "//.collect()"
   ],
   "id": "3da2d28626ed6b5f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "percentageNSFWPosts: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[10] at mapValues at <console>:39\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:09:29.850153Z",
     "start_time": "2025-02-13T12:09:29.000213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "//PART 2: Aggregate on temporal dimension and obtain average sentiment in comments and percentage of comments classified as NSFW\n",
    "//Comments are: (id,subreddit.id,subreddit.name,subreddit.nsfw,created_utc,permalink,body,sentiment,score)\n",
    "\n",
    "val avgSentimentWithNSFWComment = rddComments.map(x => (x._5, (if (x._4) 1 else 0, 1, x._8))) //(created_utc, nsfw flag, number of posts, sentiment)\n",
    ".reduceByKey((a, b) => (\n",
    "    a._1 + b._1,  // Sum NSFW counters\n",
    "    a._2 + b._2,  // Sum total post counters\n",
    "    a._3 + b._3   // Sum sentiment values\n",
    "))\n",
    ".mapValues(reduced => {\n",
    "    val (nsfwCount, totalCount, totalSentiment) = reduced\n",
    "    val nsfwPercentage = (nsfwCount * 100.0) / totalCount\n",
    "    val avgSentiment = totalSentiment / totalCount\n",
    "    (\"Percentage NSFW Comments: \"+ nsfwPercentage + \"%\", \"Average Sentiment: \" + avgSentiment)\n",
    "})\n",
    "//.coalesce(1) //riduzione a singola partizione (per analisi, con singola partizione abbiamo singolo file da analizzare)\n",
    "//.collect()"
   ],
   "id": "4fb46ae28cd213bf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgSentimentWithNSFWComment: org.apache.spark.rdd.RDD[(Int, (String, String))] = MapPartitionsRDD[13] at mapValues at <console>:37\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:09:32.360120Z",
     "start_time": "2025-02-13T12:09:31.827492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val path_output = \"../../../../output/covidPostCommentResults\"\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())"
   ],
   "id": "aa9e94559d58d738",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_output: String = ../../../../output/covidPostCommentResults\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T12:09:42.047852Z",
     "start_time": "2025-02-13T12:09:33.428645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// PART 3: Join on Temporal Dimension and then write the output\n",
    "\n",
    "val finalResult = percentageNSFWPosts.join(avgSentimentWithNSFWComment) \n",
    "//with the join i get an Array[(Int,(String,(String, String)))], in order to properly write it, i map it first then change it\n",
    ".map(x => (x._1, x._2._1, x._2._2._1, x._2._2._2))\n",
    ".coalesce(1) //riduzione a singola partizione (per analisi, con singola partizione abbiamo singolo file da analizzare)\n",
    ".toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(path_output)"
   ],
   "id": "afc6dfa8abb55193",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalResult: Unit = ()\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
