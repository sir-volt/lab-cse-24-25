{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-05T15:58:14.176018Z",
     "start_time": "2025-02-05T15:58:13.905667Z"
    }
   },
   "source": "import org.apache.spark",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:58:15.573558Z",
     "start_time": "2025-02-05T15:58:15.307817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val path_to_datasets = \"../../../../datasets/big/\"\n",
    "\n",
    "val path_ml_posts = path_to_datasets + \"the-reddit-covid-dataset-posts.csv\"\n",
    "val path_ml_comments = path_to_datasets + \"the-reddit-covid-dataset-comments.csv\""
   ],
   "id": "e3447d001ccfc3b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_to_datasets: String = ../../../../datasets/big/\r\n",
       "path_ml_posts: String = ../../../../datasets/big/the-reddit-covid-dataset-posts.csv\r\n",
       "path_ml_comments: String = ../../../../datasets/big/the-reddit-covid-dataset-comments.csv\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f82e1326f40e4ea5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:58:17.282118Z",
     "start_time": "2025-02-05T15:58:16.920303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.util.Calendar\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "object CovidConversationsParser {\n",
    "\n",
    "    val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "    val pipeRegex = \"\\\\|(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "    val quotes = \"\\\"\"\n",
    "\n",
    "    /** Convert from timestamp (String) to year (Int) */\n",
    "    def yearFromTimestamp(timestamp: String): Int = {\n",
    "        val cal = Calendar.getInstance()\n",
    "        cal.setTimeInMillis(timestamp.trim.toLong * 1000L)\n",
    "        cal.get(Calendar.YEAR)\n",
    "    }\n",
    "    \n",
    "    /** Combines splits created into their respective string values */\n",
    "    def combineText(input: Array[String], initialSplit: Int): (String, Int) = {\n",
    "        if (initialSplit >= input.length) return (\"\", initialSplit)\n",
    "    \n",
    "        var fullText = input(initialSplit)\n",
    "        var nSplit = initialSplit + 1\n",
    "    \n",
    "        if (fullText.startsWith(quotes)) {\n",
    "            while (nSplit < input.length && !input(nSplit).trim.endsWith(quotes)) {\n",
    "                fullText += input(nSplit).trim\n",
    "                nSplit += 1\n",
    "            }\n",
    "            if (nSplit < input.length) {\n",
    "                fullText += input(nSplit).trim\n",
    "                nSplit += 1\n",
    "            }\n",
    "        }\n",
    "        (fullText, nSplit)\n",
    "    }\n",
    "\n",
    "    /** Function to parse reddit posts\n",
    "    *\n",
    "    *  @param line line that has to be parsed\n",
    "    *  @return tuple containing id,subreddit.id,subreddit.name,subreddit.nsfw,created_utc,permalink,domain,url,selftext,title,score. none in case of input errors\n",
    "    */\n",
    "    def parseRedditPost(line: String): Option[(String, String, String, Boolean, Int, String, String, java.io.Serializable, java.io.Serializable, String, String)] = {\n",
    "        try {\n",
    "            val input = line.split(commaRegex)\n",
    "            val url = if(input(8).trim.nonEmpty) input(8).trim else None\n",
    "            val selftext = if(input(9).trim.nonEmpty) combineText(input, 9)._1 else None\n",
    "            var n = combineText(input, 9)._2\n",
    "            val title = combineText(input, n)._1\n",
    "            n = combineText(input, n)._2\n",
    "            val score = combineText(input, n)._1\n",
    "            Some((input(1).trim, input(2).trim, input(3).trim, input(4).trim.toBoolean, yearFromTimestamp(input(5)), input(6).trim, input(7).trim, url, selftext, title, score))   \n",
    "        } catch {\n",
    "            case _: Exception => None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /** Function to parse reddit comments\n",
    "    *\n",
    "    *  @param line line that has to be parsed\n",
    "    *  @return tuple containing id,subreddit.id,subreddit.name,subreddit.nsfw,created_utc,permalink,body,sentiment,score. none in case of input errors\n",
    "    */\n",
    "    def parseRedditComment(line: String): Option[(String, String, String, Boolean, Int, String, String, Double, Int)] = {\n",
    "        try {\n",
    "            val input = line.split(commaRegex)\n",
    "            val body = combineText(input, 7)._1\n",
    "            var n = combineText(input, 7)._2\n",
    "            val sentiment = combineText(input, n)._1.toDouble\n",
    "            n = combineText(input, n)._2\n",
    "            val score = combineText(input, n)._1.toInt\n",
    "            Some((input(1).trim, input(2).trim, input(3).trim, input(4).trim.toBoolean, yearFromTimestamp(input(5)), input(6).trim, body, sentiment, score))\n",
    "        } catch {\n",
    "            case _: Exception => None\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "id": "c887d7e89a51786e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.Calendar\r\n",
       "import org.apache.spark.sql.SaveMode\r\n",
       "import org.apache.spark.HashPartitioner\r\n",
       "defined object CovidConversationsParser\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:58:21.587212Z",
     "start_time": "2025-02-05T15:58:21.340607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "//case class CovidPost(id:String, subredditId:String, subredditName:String, subredditNSFW:boolean, created_UTC: Calendar,permalink:String, domain:String, URL:String, selftext:Option[String], title:String, score:Int)\n",
    "\n",
    "//case class CovidComment(id:String, subredditId:String, subredditName:String, subredditNSFW:boolean, created_UTC: Calendar,permalink:String, body:String, sentiment:Float, score:Int)"
   ],
   "id": "9b5e0aa76d4631f8",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "517000d0f73409c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:55:59.209914Z",
     "start_time": "2025-02-05T15:55:53.181437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "//per recuperare dei samples dei files\n",
    "sc.textFile(path_ml_posts).sample(false, 0.2).coalesce(1).toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(\"../../../../datasets/\")"
   ],
   "id": "6ef50ca52909c10e",
   "outputs": [
    {
     "ename": "org.apache.hadoop.mapred.InvalidInputException",
     "evalue": " Input path does not exist: file:/C:/Users/admin/Desktop/Esercizi_Big_Data/lab-cse-24-25/datasets/big/the-reddit-covid-dataset-posts.csv\r",
     "output_type": "error",
     "traceback": [
      "org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/Users/admin/Desktop/Esercizi_Big_Data/lab-cse-24-25/datasets/big/the-reddit-covid-dataset-posts.csv\r",
      "  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r",
      "  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r",
      "  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r",
      "  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r",
      "  at scala.Option.getOrElse(Option.scala:189)\r",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r",
      "  at scala.Option.getOrElse(Option.scala:189)\r",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r",
      "  at org.apache.spark.rdd.PartitionwiseSampledRDD.getPartitions(PartitionwiseSampledRDD.scala:58)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r",
      "  at scala.Option.getOrElse(Option.scala:189)\r",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r",
      "  at org.apache.spark.rdd.DefaultPartitionCoalescer$PartitionLocations.getAllPrefLocs(CoalescedRDD.scala:197)\r",
      "  at org.apache.spark.rdd.DefaultPartitionCoalescer$PartitionLocations.<init>(CoalescedRDD.scala:190)\r",
      "  at org.apache.spark.rdd.DefaultPartitionCoalescer.coalesce(CoalescedRDD.scala:391)\r",
      "  at org.apache.spark.rdd.CoalescedRDD.getPartitions(CoalescedRDD.scala:90)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r",
      "  at scala.Option.getOrElse(Option.scala:189)\r",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r",
      "  at scala.Option.getOrElse(Option.scala:189)\r",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r",
      "  at scala.Option.getOrElse(Option.scala:189)\r",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r",
      "  at org.apache.spark.sql.execution.datasources.WriteFilesExec.doExecuteWrite(WriteFiles.scala:77)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeWrite$1(SparkPlan.scala:235)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeWrite(SparkPlan.scala:231)\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:305)\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r",
      "  at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r",
      "  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r",
      "  ... 39 elided\r",
      "Caused by: java.io.IOException: Input path does not exist: file:/C:/Users/admin/Desktop/Esercizi_Big_Data/lab-cse-24-25/datasets/big/the-reddit-covid-dataset-posts.csv\r",
      "  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r",
      "  ... 107 more\r",
      ""
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:20:39.009643Z",
     "start_time": "2025-02-05T15:20:37.715730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "val pipeRegex = \"\\\\|(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "val quotes = \"\\'\"\n",
    "\n",
    "def yearFromTimestamp(timestamp: String): Int = {\n",
    "    val cal = Calendar.getInstance()\n",
    "cal.setTimeInMillis(timestamp.trim.toLong * 1000L)\n",
    "cal.get(Calendar.YEAR)\n",
    "}\n",
    "\n",
    "def combineText(input: Array[String], initialSplit: Int): (String, Int) = {\n",
    "    if (initialSplit >= input.length) return (\"\", initialSplit)\n",
    "\n",
    "    var fullText = input(initialSplit)\n",
    "    var nSplit = initialSplit + 1\n",
    "\n",
    "    if (fullText.startsWith(quotes)) {\n",
    "        while (nSplit < input.length && !input(nSplit).trim.endsWith(quotes)) {\n",
    "            fullText += input(nSplit).trim\n",
    "            nSplit += 1\n",
    "            }\n",
    "        if (nSplit < input.length) {\n",
    "            fullText += input(nSplit).trim\n",
    "            nSplit += 1\n",
    "        }\n",
    "}\n",
    "    (fullText, nSplit)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "val value = \"post,qftuah,2qhh9,quebec,false,1635206276,https://old.reddit.com/r/Quebec/comments/qftuah/le_saviezvous_sous_ios_151_vous_pouvez_rajouter/,self.quebec,,'https://support.apple.com/en-ca/HT212752 Avec la nouvelle version de la passe covid qui est maintenant conforme au standard SMART Health card, il est possible d'utiliser le scanneur QR intégré au panneau de contrôle de votre iPhone pour scanner votre nouvelle passe et rajouter votre preuve vaccinal dans l'application *Santé*. C'est une nouvelle fonctionalité qui est arrivé dans iOS 15.0. Aujourd'hui avec la sortie de iOS 15.1, il est maintenant possible de la rajouter au *Portefeuille d'Apple*. Il n'est donc plus nécessaire d'utiliser l'application VaxiCode, sauf si peut-être vous la trouvez plus facile pour maintenir les passes de toute votre famille. Vous devez aller télécharger votre nouvelle preuve vacinnal disponible sur le site web du gouvernement comme vous l'avez fait pour la première fois car votre ancien code n'est pas conforme. Il y a 0 différence entre la passe ''pour utilisation au Québec'' et la passe ''pour utilisation hors du Québec''. C'est juste que le gouvernement joue une game de relation publique pour ne pas emmerder les gens et pour ne pas paraître comme si ils n'ont pas gaspillé une couple de millions pour ne pas l'avoir fait correct la première fois.',Le saviez-vous ? Sous iOS 15.1 vous pouvez rajouter la passe Covid au Portefeuille d'Apple? Plus besoin de VaxiCode!,0\"\n",
    "val input = value.split(commaRegex)\n",
    "var url = if(input(8).trim.nonEmpty) input(8).trim else None\n",
    "val selftext = if(input(9).trim.nonEmpty) combineText(input, 9)._1 else None\n",
    "var n = combineText(input, 9)._2\n",
    "val title = combineText(input, n)._1\n",
    "n = combineText(input, n)._2\n",
    "val score = combineText(input, n)._1"
   ],
   "id": "72a3df9388606c92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.Calendar\r\n",
       "import org.apache.spark.sql.SaveMode\r\n",
       "import org.apache.spark.HashPartitioner\r\n",
       "commaRegex: String = ,(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)\r\n",
       "pipeRegex: String = \\|(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)\r\n",
       "quotes: String = '\r\n",
       "yearFromTimestamp: (timestamp: String)Int\r\n",
       "combineText: (input: Array[String], initialSplit: Int)(String, Int)\r\n",
       "value: String = post,qftuah,2qhh9,quebec,false,1635206276,https://old.reddit.com/r/Quebec/comments/qftuah/le_saviezvous_sous_ios_151_vous_pouvez_rajouter/,self.quebec,,'https://support.apple.com/en-ca/HT212752 Avec la nouvelle version de la passe covid qui est maintenant conforme au standard SMART Health card, il est possible d'utiliser le scanneur QR intégré au panneau de contrôle de votre iPhone pour scanner votre nouvelle passe et rajouter votre preu...\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:20:47.638025Z",
     "start_time": "2025-02-05T15:20:47.002843Z"
    }
   },
   "cell_type": "code",
   "source": "(input(1).trim, input(2).trim, input(3).trim, input(4).trim.toBoolean, yearFromTimestamp(input(5)), input(6).trim, input(7).trim, url, selftext, title, score)",
   "id": "ef8bd011160741f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: (String, String, String, Boolean, Int, String, String, java.io.Serializable, java.io.Serializable, String, String) = (qftuah,2qhh9,quebec,false,2021,https://old.reddit.com/r/Quebec/comments/qftuah/le_saviezvous_sous_ios_151_vous_pouvez_rajouter/,self.quebec,None,'https://support.apple.com/en-ca/HT212752 Avec la nouvelle version de la passe covid qui est maintenant conforme au standard SMART Health cardil est possible d'utiliser le scanneur QR intégré au panneau de contrôle de votre iPhone pour scanner votre nouvelle passe et rajouter votre preuve vaccinal dans l'application *Santé*. C'est une nouvelle fonctionalité qui est arrivé dans iOS 15.0. Aujourd'hui avec la sortie de iOS 15.1il est maintenant possible de la rajouter au *Portefeuille d'Apple*. Il n'est donc plus nécessaire d...\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T20:14:20.247507Z",
     "start_time": "2025-02-04T20:14:19.333338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val commentValue = \"comment,hi1vs7i,2qhov,vancouver,false,1635206397,https://old.reddit.com/r/vancouver/comments/qft2x3/bc_takes_note_as_new_zealand_moves_to_ban/hi1vs7i/,'Didn't stop prices there though. New Zealand and Canada grew at about the same rate through COVID. I agree that non-resident ownership should be stopped, but it's also probably not going to change anything.',0.1887,32\"\n",
    "val input = commentValue.split(commaRegex)\n",
    "val body = combineText(input, 7)._1\n",
    "var n = combineText(input, 7)._2\n",
    "val sentiment = combineText(input, n)._1.toDouble\n",
    "n = combineText(input, n)._2\n",
    "val score = combineText(input, n)._1.toInt"
   ],
   "id": "32eee082e5a2a05a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "commentValue: String = comment,hi1vs7i,2qhov,vancouver,false,1635206397,https://old.reddit.com/r/vancouver/comments/qft2x3/bc_takes_note_as_new_zealand_moves_to_ban/hi1vs7i/,'Didn't stop prices there though. New Zealand and Canada grew at about the same rate through COVID. I agree that non-resident ownership should be stopped, but it's also probably not going to change anything.',0.1887,32\r\n",
       "input: Array[String] = Array(comment, hi1vs7i, 2qhov, vancouver, false, 1635206397, https://old.reddit.com/r/vancouver/comments/qft2x3/bc_takes_note_as_new_zealand_moves_to_ban/hi1vs7i/, 'Didn't stop prices there though. New Zealand and Canada grew at about the same rate through COVID. I agree that non-resident ownership should be stopped, \" but it's also probably not going to change anything.'\", 0.1...\r\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-04T20:14:48.133437Z",
     "start_time": "2025-02-04T20:14:47.350622Z"
    }
   },
   "cell_type": "code",
   "source": "(input(1).trim, input(2).trim, input(3).trim, input(4).trim.toBoolean, yearFromTimestamp(input(5)), input(6).trim, body, sentiment, score)",
   "id": "afc6dfa8abb55193",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res49: (String, String, String, Boolean, Int, String, String, Double, Int) = (hi1vs7i,2qhov,vancouver,false,2021,https://old.reddit.com/r/vancouver/comments/qft2x3/bc_takes_note_as_new_zealand_moves_to_ban/hi1vs7i/,'Didn't stop prices there though. New Zealand and Canada grew at about the same rate through COVID. I agree that non-resident ownership should be stoppedbut it's also probably not going to change anything.',0.1887,32)\r\n"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 95
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
