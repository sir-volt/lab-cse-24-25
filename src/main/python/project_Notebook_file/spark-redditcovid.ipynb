{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-11T20:40:12.177830Z",
     "start_time": "2025-02-11T20:39:54.257809Z"
    }
   },
   "source": "import org.apache.spark",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-T2P39KLE:4040\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1739306401114)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T20:40:16.426832Z",
     "start_time": "2025-02-11T20:40:15.024709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val path_to_bigDatasets = \"../../../../datasets/big/\"\n",
    "val path_to_datasets = \"../../../../datasets/\"\n",
    "\n",
    "val path_Fullml_posts = path_to_bigDatasets + \"the-reddit-covid-dataset-posts.csv\"\n",
    "val path_Fullml_comments = path_to_bigDatasets + \"the-reddit-covid-dataset-comments.csv\"\n",
    "\n",
    "val path_sample_posts = path_to_datasets + \"postsSample.csv\"\n",
    "val path_sample_comments = path_to_datasets + \"commentsSample.csv\""
   ],
   "id": "e3447d001ccfc3b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_to_bigDatasets: String = ../../../../datasets/big/\r\n",
       "path_to_datasets: String = ../../../../datasets/\r\n",
       "path_Fullml_posts: String = ../../../../datasets/big/the-reddit-covid-dataset-posts.csv\r\n",
       "path_Fullml_comments: String = ../../../../datasets/big/the-reddit-covid-dataset-comments.csv\r\n",
       "path_sample_posts: String = ../../../../datasets/postsSample.csv\r\n",
       "path_sample_comments: String = ../../../../datasets/commentsSample.csv\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f82e1326f40e4ea5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T20:40:19.936139Z",
     "start_time": "2025-02-11T20:40:17.957027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import java.util.Calendar\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "object CovidConversationsParser {\n",
    "\n",
    "    val commaSplit = \",\"\n",
    "    val quotes = \"\\\"\"\n",
    "\n",
    "    /** Convert from timestamp (String) to year (Int) */\n",
    "    def yearFromTimestamp(timestamp: String): Int = {\n",
    "        val cal = Calendar.getInstance()\n",
    "        cal.setTimeInMillis(timestamp.trim.toLong * 1000L)\n",
    "        cal.get(Calendar.YEAR)\n",
    "    }\n",
    "\n",
    "    /** Function to parse reddit posts\n",
    "    *\n",
    "    *  @param line line that has to be parsed\n",
    "    *  @return tuple containing id,subreddit.id,subreddit.name,subreddit.nsfw,created_utc,permalink,domain,url,selftext,title,score. none in case of input errors\n",
    "    */\n",
    "    def parseRedditPost(line: String): Option[(String, String, String, Boolean, Int, String, String, String, String, String, Int)] = {\n",
    "        try {\n",
    "            val input = line.split(commaSplit)\n",
    "            if(input.size != 12) {\n",
    "                return Some((\"\", \"\", \"\", false, -1, \"\", \"\", \"\", \"\", \"\", -1))\n",
    "            }\n",
    "            var url = \"None\"\n",
    "            if(input(8).trim.nonEmpty) {\n",
    "                url = input(8).trim\n",
    "            }\n",
    "            var selftext = \"None\"\n",
    "            if(input(9).trim.nonEmpty) {\n",
    "                selftext = input(9).trim\n",
    "            }\n",
    "            val number = input(11).trim.replaceAll(quotes, \"\")\n",
    "            Some((input(1).trim, input(2).trim, input(3).trim, input(4).trim.toBoolean, yearFromTimestamp(input(5)), input(6).trim, input(7).trim, url, selftext, input(10).trim , number.toInt))\n",
    "        } catch {\n",
    "            case _: Exception => None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /** Function to parse reddit comments\n",
    "    *\n",
    "    *  @param line line that has to be parsed\n",
    "    *  @return tuple containing id,subreddit.id,subreddit.name,subreddit.nsfw,created_utc,permalink,body,sentiment,score. none in case of input errors\n",
    "    */\n",
    "    def parseRedditComment(line: String): Option[(String, String, String, Boolean, Int, String, String, Double, Int)] = {\n",
    "        try {\n",
    "            val input = line.split(commaSplit)\n",
    "            if(input.size != 10) {\n",
    "                return Some((\"\", \"\", \"\", false, -1, \"\", \"\", -0.1, -1))\n",
    "            }\n",
    "            val number = input(9).trim.replaceAll(quotes, \"\")\n",
    "            Some((input(1).trim, input(2).trim, input(3).trim, input(4).trim.toBoolean,yearFromTimestamp(input(5)), input(6).trim, input(7).trim, input(8).trim.toDouble, number.toInt))\n",
    "        } catch {\n",
    "            case _: Exception => None\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "id": "c887d7e89a51786e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.Calendar\r\n",
       "import org.apache.spark.sql.SaveMode\r\n",
       "import org.apache.spark.HashPartitioner\r\n",
       "defined object CovidConversationsParser\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9b5e0aa76d4631f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2a57888c5e50b341",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val array = Array(\"\",\"10200SB1169ham002- 2 -LRB102 04980 LNS 30154 a1\",\n",
    "                  \"post,qftn1l,4ir4l2,deplatformed_,false,1635205611,https://old.reddit.com/r/Deplatformed_/comments/qftn1l/how_many_lives_have_been_lost_due_to_covid19/,freeworldnews.tv,https://freeworldnews.tv/watch?id=617610dbb48cdd1309da36bb,,How many lives have been lost due to Covid-19 'treatment hesitancy' from the left?,1\",\n",
    "                  \"post,qftm4e,2qh1i,askreddit,false,1635205525,https://old.reddit.com/r/AskReddit/comments/qftm4e/does_anyone_think_this_covid_drama_will_go_away/,self.askreddit,,[removed],Does anyone think this covid drama will go away? Or will it stick around now like the common cold?,1\", \"- Mission Memorial Hospital, Chilliwack General Hospital (Fraser Health);\",\n",
    "                  \"post,qfsoln,2qs1t,newyorkcity,false,1635202606,https://old.reddit.com/r/newyorkcity/comments/qfsoln/live_now_78pm_pix11_holds_their_2021_nyc_mayoral/,pix11.com,https://pix11.com/news/politics/new-york-elections/pix-on-politics-adams-sliwa-to-face-off-in-nyc-mayoral-forum-monday/,,\\\"Live Now (7-8PM): Pix11 holds their 2021 NYC mayoral forum, with frontrunner candidates Eric Adams (D), and Curtis Sliwa (R) answering questions on city issues such as COVID-19, vaccine mandates, schools, crime, youth violence and much more. (10/25/2021)\\\",1\")\n",
    "val testRDDPost = sc.parallelize(array, 2)\n",
    "val testParser = testRDDPost.flatMap(CovidConversationsParser.parseRedditPost)\n",
    "    .filter(x => x != (\"\", \"\", \"\", false, -1, \"\", \"\", \"\", \"\", \"\", -1))\n",
    "print(testParser.first())"
   ],
   "id": "1f13bba1ea63b876",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val commentsArray = Array(\"They were very specific about what they were and weren't studying.  The article you linked from  Nature was less specific.  Your summary is completely mischaracterizing the study you're trying to use as evidence for your position.\\\",0.8976,1\", \"======================\", \"comment,hi1vjyo,2y77d,antiwork,false,1635206288,https://old.reddit.com/r/antiwork/comments/qft1w1/i_work_for_a_lab_doing_covid_testing_everyday_ive/hi1vjyo/,Can I come love with you? I can do Covid testing for free and take up very little space.,0.8176,11\"\n",
    "        ,\"If there is an issue with this post and it breaks the rules, please use the report button AND message the Moderators.\",\"comment,hi1uwl8,2ra72,nursing,false,1635205980,https://old.reddit.com/r/nursing/comments/qfkl6h/i_wish_obituaries_for_unvaccinated_that_die_of/hi1uwl8/,\\\"Yeah, apparently the cancer is what did Colin Powell...and not, you know, the Covid breakthrough from his immune system being absolutely shot.\\\",-0.25,18\", \"comment,hi1uj9b,3fmhe,enough_sanders_spam,false,1635205810,https://old.reddit.com/r/Enough_Sanders_Spam/comments/qfb1vm/mondays_general_discussion_roundtable_10252021/hi1uj9b/,\\\"&gt;[The most hate crimes in the U.S. since 2001 were reported last year, according to newly released data from the FBI that also revealed a stunning 76% increase in anti-Asian hate crimes.](https://www.huffpost.com/entry/fbi-2020-hate-crimes-asian-americans_n_6176fd81e4b010d933148e55)\", \"comment,hi1tnwd,2reak,marriage,false,1635205404,https://old.reddit.com/r/Marriage/comments/qft3pf/should_i_feel_guilty_for_hanging_out_with_friends/hi1tnwd/,Hahaha well that is a perspective to look at and understand. I hope his insecurities aren’t as full throttle as yours. Unfortunately fuckfests aren’t possible due to Covid restrictions in my state right now.,0.5267,3\")\n",
    "val testRDDComment = sc.parallelize(commentsArray, 2)\n",
    "val testCommentParser = testRDDComment.flatMap(CovidConversationsParser.parseRedditComment)\n",
    "                .filter(x => x != (\"\", \"\", \"\", false, -1, \"\", \"\", -0.1, -1))\n",
    "println(testCommentParser.first())"
   ],
   "id": "4ca887ae593a003f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cd91a071764e682e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val sampleFile = path_to_datasets + \"sampleTest.csv\"\n",
    "val sampleFileRDD = sc.textFile(sampleFile)\n",
    "val parserTest = sampleFileRDD.flatMap(CovidConversationsParser.parseRedditPost)\n",
    "                 .filter(x => x != (\"\", \"\", \"\", false, -1, \"\", \"\", \"\", \"\", \"\", -1))\n",
    "//println(sampleFileRDD.first().getClass.getSimpleName)\n",
    "println(parserTest.count())"
   ],
   "id": "72ef415efea5f847",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val sampleCommentFile = path_to_datasets + \"sampleCommentTest.csv\"\n",
    "val sampleCommentRDD = sc.textFile(sampleCommentFile)\n",
    "val parserCommentTest = sampleCommentRDD .flatMap(CovidConversationsParser.parseRedditComment)\n",
    "                 .filter(x => x != (\"\", \"\", \"\", false, -1, \"\", \"\", -0.1, -1))\n",
    "//println(sampleFileRDD.first().getClass.getSimpleName)\n",
    "println(parserCommentTest.count())"
   ],
   "id": "ee51d50d79d17403",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "517000d0f73409c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "//per recuperare dei samples dei files\n",
    "//sc.textFile(path_Fullml_posts).sample(false, 0.05).coalesce(1).toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(\"../../../../datasets/sample\")\n",
    "sc.textFile(path_Fullml_comments).sample(false, 0.02).coalesce(1).toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(\"../../../../datasets/sample\")"
   ],
   "id": "6ef50ca52909c10e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "14c684e2f81c5981",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "32a8ea18bd746b6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T20:40:32.628141Z",
     "start_time": "2025-02-11T20:40:28.948707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "/*getting RDDs of Comments and Posts*/\n",
    "val rddPosts = sc.textFile(path_sample_posts).flatMap(CovidConversationsParser.parseRedditPost).filter(x => x != (\"\",\"\",\"\",false,-1,\"\",\"\",\"\",\"\",\"\",-1))\n",
    "val rddComments = sc.textFile(path_sample_comments).flatMap(CovidConversationsParser.parseRedditComment)\n",
    ".filter(x => x != (\"\", \"\", \"\", false, -1, \"\", \"\", -0.1, -1))\n",
    "rddComments.first()"
   ],
   "id": "72a3df9388606c92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddPosts: org.apache.spark.rdd.RDD[(String, String, String, Boolean, Int, String, String, String, String, String, Int)] = MapPartitionsRDD[3] at filter at <console>:32\r\n",
       "rddComments: org.apache.spark.rdd.RDD[(String, String, String, Boolean, Int, String, String, Double, Int)] = MapPartitionsRDD[7] at filter at <console>:34\r\n",
       "res0: (String, String, String, Boolean, Int, String, String, Double, Int) = (hi1vjyo,2y77d,antiwork,false,2021,https://old.reddit.com/r/antiwork/comments/qft1w1/i_work_for_a_lab_doing_covid_testing_everyday_ive/hi1vjyo/,Can I come love with you? I can do Covid testing for free and take up very little space.,0.8176,11)\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "be7643bf11864cf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T20:40:36.826362Z",
     "start_time": "2025-02-11T20:40:34.477510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "//PART 1: Aggregate on temporal dimension and obtain percentage of posts classified as NSFW\n",
    "//Posts are :(id,subreddit.id,subreddit.name,subreddit.nsfw,created_utc,permalink,domain,url,selftext,title,score)\n",
    "//(x._1, x._3, x._4)\n",
    "val percentageNSFWPosts = rddPosts.map(x => (x._5, x._4))  // (created_utc, nsfw flag)\n",
    ".groupByKey()\n",
    ".mapValues({case (nsfwCount) =>\n",
    "    val totalPostsAtTime = nsfwCount.size\n",
    "    val totNSFWPost = nsfwCount.count(el => el == true)\n",
    "    ((totNSFWPost * 100) / totalPostsAtTime).toDouble\n",
    "})\n",
    "//.coalesce(1) //riduzione a singola partizione (per analisi, con singola partizione abbiamo singolo file da analizzare)\n",
    "//.collect()"
   ],
   "id": "ef8bd011160741f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "percentageNSFWPosts: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[10] at mapValues at <console>:33\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val percentageNSFWPosts = rddPosts.map(x => (x._5, (x._1, x._3, if (x._4) 1 else 0))) // (timestamp, (id, subreddit_name, nsfw flag))\n",
    "    .aggregateByKey((0, 0))( // (nsfwCount, totalCount)\n",
    "    // Accumulator for each partition\n",
    "    (acc, value) => (acc._1 + value._3, acc._2 + 1),\n",
    "    // Combiner for merging partitions\n",
    "    (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    ").map { case (timestamp, (nsfwCount, totalCount)) =>\n",
    "    val percentage = (nsfwCount.toDouble / totalCount) * 100\n",
    "    (timestamp, rddPosts.filter(_._5 == timestamp).first()._1,  // id\n",
    "    rddPosts.filter(_._5 == timestamp).first()._3,  // subreddit_name\n",
    "    percentage)\n",
    "}.coalesce(1) //riduzione a singola partizione (per analisi, con singola partizione abbiamo singolo file da analizzare)\n",
    ".collect()"
   ],
   "id": "8430eb481a126db3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T20:40:47.044626Z",
     "start_time": "2025-02-11T20:40:45.747367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "//PART 2: Aggregate on temporal dimension and obtain average sentiment in comments and percentage of comments classified as NSFW\n",
    "//Comments are: (id,subreddit.id,subreddit.name,subreddit.nsfw,created_utc,permalink,body,sentiment,score)\n",
    "val avgSentimentWithNSFWPost = rddComments.map(x => (x._5, (x._4, x._8)))\n",
    ".groupByKey()\n",
    ".mapValues({case (nsfwAndSentimentCount) =>\n",
    "    val totalPostsAtTime = nsfwAndSentimentCount.size\n",
    "    val totNSFWPost = nsfwAndSentimentCount.count(el => el._1 == true)\n",
    "    val percentage = ((totNSFWPost * 100) / totalPostsAtTime).toDouble\n",
    "    val totSentiment = nsfwAndSentimentCount.map(x => x._2).foldLeft(0.0)(_ + _)\n",
    "    (percentage, totSentiment / totalPostsAtTime)\n",
    "})\n",
    "//.coalesce(1) //riduzione a singola partizione (per analisi, con singola partizione abbiamo singolo file da analizzare)\n",
    "//.collect()"
   ],
   "id": "32eee082e5a2a05a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgSentimentWithNSFWPost: org.apache.spark.rdd.RDD[(Int, (Double, Double))] = MapPartitionsRDD[13] at mapValues at <console>:32\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "/*.mapValues({case (nsfwCount, sentimentCount) =>\n",
    "val totalPostsAtTime = nsfwCount.size\n",
    "val totNSFWPost = nsfwCount.count(el => el == true)\n",
    "((totNSFWPost * 100) / totalPostsAtTime).toDouble\n",
    "})*/\n",
    "/*\n",
    "val avgSentimentWithNSFWPost = rddComments.map(x => (x._5, (if (x._4) 1 else 0, x._8)))  // (created_utc, (nsfw flag, sentiment))\n",
    ".reduceByKey((a, b) => (a._1 + b._1, a._2 + b._2))  // aggregate NSFW count and sentiment\n",
    ".mapValues(avgSentimentAndNSFWCount => {\n",
    "val totalCommentsAtTime = rddComments.filter(y => y._5 == y._5).count()\n",
    "val nsfwPercentage = (avgSentimentAndNSFWCount._1.toDouble / totalCommentsAtTime) * 100\n",
    "val avgSentiment = avgSentimentAndNSFWCount._2 / totalCommentsAtTime\n",
    "(nsfwPercentage, avgSentiment)\n",
    "})*/"
   ],
   "id": "4b059a22dc9e3638"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T20:41:16.514882Z",
     "start_time": "2025-02-11T20:41:10.699954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// PART 3: Join on Temporal Dimension\n",
    "val finalResult = percentageNSFWPosts.join(avgSentimentWithNSFWPost)\n",
    ".coalesce(1) //riduzione a singola partizione (per analisi, con singola partizione abbiamo singolo file da analizzare)\n",
    ".collect()"
   ],
   "id": "afc6dfa8abb55193",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalResult: Array[(Int, (Double, (Double, Double)))] = Array((2020,(1.0,(0.0,0.15337666033167532))), (2021,(1.0,(0.0,0.017297035915188295))))\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
