{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee",
   "metadata": {
    "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee"
   },
   "source": [
    "# 103 Spark optimizations\n",
    "\n",
    "The goal of this lab is to understand some of the optimization mechanisms of Spark.\n",
    "\n",
    "- [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
    "- [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a037caa76dc389a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:34:50.752064Z",
     "start_time": "2024-10-20T16:34:42.694703Z"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f504e515fbb5fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "// DO NOT EXECUTE - this is needed just to avoid showing errors in the following cells\n",
    "val sc = spark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:35:52.972776Z",
     "start_time": "2024-10-20T16:35:52.357262Z"
    },
    "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "// WEATHER structure: (usaf,wban,year,month,day,airTemperature,airTemperatureQuality)\n",
    "def parseWeather(row:String) = {\n",
    "    val usaf = row.substring(4,10)\n",
    "    val wban = row.substring(10,15)\n",
    "    val year = row.substring(15,19)\n",
    "    val month = row.substring(19,21)\n",
    "    val day = row.substring(21,23)\n",
    "    val airTemperature = row.substring(87,92)\n",
    "    val airTemperatureQuality = row.charAt(92)\n",
    "\n",
    "    (usaf,wban,year,month,day,airTemperature.toInt/10,airTemperatureQuality == '1')\n",
    "}\n",
    "\n",
    "// STATION structure: (usaf,wban,city,country,state,latitude,longitude,elevation,date_begin,date_end) \n",
    "def parseStation(row:String) = {\n",
    "    def getDouble(str:String) : Double = {\n",
    "        if (str.isEmpty)\n",
    "            return 0\n",
    "        else\n",
    "            return str.toDouble\n",
    "    }\n",
    "    val columns = row.split(\",\").map(_.replaceAll(\"\\\"\",\"\"))\n",
    "    val latitude = getDouble(columns(6))\n",
    "    val longitude = getDouble(columns(7))\n",
    "    val elevation = getDouble(columns(8))\n",
    "    (columns(0),columns(1),columns(2),columns(3),columns(4),latitude,longitude,elevation,columns(9),columns(10))  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c02bd-4c8f-4cc2-9a13-544da7c6544d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:35:57.630942Z",
     "start_time": "2024-10-20T16:35:56.554809Z"
    }
   },
   "outputs": [],
   "source": [
    "val rddWeather = sc.\n",
    "  textFile(\"../../../../datasets/big/weather-sample10.txt\").\n",
    "  map(x => parseWeather(x))\n",
    "val rddStation = sc.\n",
    "  textFile(\"../../../../datasets/weather-stations.csv\").\n",
    "  map(x => parseStation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b49ee-6852-4025-9e55-3950ff937680",
   "metadata": {
    "id": "ef4b49ee-6852-4025-9e55-3950ff937680"
   },
   "source": [
    "## 103-1 Simple job optimization\n",
    "\n",
    "Optimize the two jobs (avg temperature and max temperature) by avoiding the repetition of the same computations and by enforcing a partitioning criteria.\n",
    "- There are multiple methods to repartition an RDD: check the ```coalesce```, ```partitionBy```, and ```repartition``` methods on the documentation and choose the best one.\n",
    "  - To create a partitioning function, you must ```import org.apache.spark.HashPartitioner``` and then define ```val p = new HashPartitioner(n)``` where ```n``` is the number of partitions to create\n",
    "- Verify your persisted data in the web UI\n",
    "- Verify the execution plan of your RDDs with ```rdd.toDebugString``` (shell only) or on the web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:10:42.682093Z",
     "start_time": "2024-10-20T15:10:41.849647Z"
    },
    "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Average temperature for every month\n",
    "rddWeather.\n",
    "  filter(_._6<999).\n",
    "  map(x => (x._4, x._6)).\n",
    "  aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2)).\n",
    "  map({case(k,v)=>(k,Math.round(v._1*100/v._2)/100.0)}).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614d5393d1a1c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:11:13.188402Z",
     "start_time": "2024-10-20T15:11:12.853137Z"
    }
   },
   "outputs": [],
   "source": [
    "// Maximum temperature for every month\n",
    "rddWeather.\n",
    "  filter(_._6<999).\n",
    "  map(x => (x._4, x._6)).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f9903-62cd-40a2-adc7-d821e924ea08",
   "metadata": {
    "id": "ee3f9903-62cd-40a2-adc7-d821e924ea08",
    "tags": []
   },
   "source": [
    "### Solution\n",
    "\n",
    "Caching:\n",
    "- It is good to cache the RDD **after** the common transformations (including the partitioning) have been carried out\n",
    "- Then, remember to reference the cached RDD\n",
    "- Check cached RDD in the Spark UI\n",
    "\n",
    "Partitioning:\n",
    "- ```coalesce``` aggregates partitions without shuffling; ```repartition``` and ```partitionBy``` force a shuffle\n",
    "- Differently from ```partitionBy```, ```coalesce``` and ```repartition``` do not associate a partitioning criteria to the RDD; thus, both jobs must reshuffle the cached data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5841b168-df48-4e00-bd6a-6160b0838bc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:44:13.170818Z",
     "start_time": "2024-10-20T15:44:13.016924Z"
    },
    "id": "5841b168-df48-4e00-bd6a-6160b0838bc1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val p = new HashPartitioner(8)\n",
    "\n",
    "val cachedRdd = rddWeather.\n",
    "  filter(_._6<999).\n",
    "  map(x => (x._4, x._6)).\n",
    "  partitionBy(p).\n",
    "  cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39309a563c13e0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:44:27.441443Z",
     "start_time": "2024-10-20T15:44:26.737831Z"
    }
   },
   "outputs": [],
   "source": [
    "// Average temperature for every month\n",
    "cachedRdd.\n",
    "  aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2)).\n",
    "  map({case(k,v)=>(k,Math.round(v._1*100/v._2)/100.0)}).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e614c67fc280f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:44:28.893796Z",
     "start_time": "2024-10-20T15:44:28.657879Z"
    }
   },
   "outputs": [],
   "source": [
    "// Maximum temperature for every month\n",
    "cachedRdd.\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377fbf30-f568-413c-9238-de139db23135",
   "metadata": {
    "id": "377fbf30-f568-413c-9238-de139db23135"
   },
   "source": [
    "## 103-2 RDD preparation\n",
    "\n",
    "Check the five possibilities to prepare the Station RDD for subsequent processing and identify the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T15:51:59.650807Z",
     "start_time": "2024-10-20T15:51:59.467316Z"
    },
    "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val p2 = new HashPartitioner(8)\n",
    "\n",
    "// _1 and _2 are the fields composing the key; _4 and _8 are country and elevation, respectively\n",
    "val rddS1 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p2).\n",
    "  cache().\n",
    "  map({case (k,v) => (k,(v._4,v._8))})\n",
    "val rddS2 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  cache().\n",
    "  partitionBy(p2)\n",
    "val rddS3 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  cache()\n",
    "val rddS4 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  partitionBy(p2).\n",
    "  cache()\n",
    "val rddS5 = rddStation.\n",
    "  map(x => (x._1 + x._2, (x._4,x._8))).\n",
    "  partitionBy(p2).\n",
    "  cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e97789d-9b91-4d68-a1df-f84e7dccc759",
   "metadata": {
    "id": "0e97789d-9b91-4d68-a1df-f84e7dccc759"
   },
   "source": [
    "### Solution\n",
    "\n",
    "- keyBy() and map() break the partitioning, thus they must be issued before partitionBy()\n",
    "- Anything that happens after cache() is not saved and must be recomputed each time;\n",
    "  thus, it is good practice to cache() as later as possible\n",
    "- rddS4 and rddS5 are the best options (the latter being less verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a093dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "// Use this to check the partitioner associated with an RDD\n",
    "rddS5.partitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed",
   "metadata": {
    "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed"
   },
   "source": [
    "## 103-3 Joining RDDs\n",
    "\n",
    "Define the join between rddWeather and rddStation and compute:\n",
    "- The maximum temperature for every city\n",
    "- The maximum temperature for every city in the UK: \n",
    "  - ```StationData.country == \"UK\"```\n",
    "- Sort the results by descending temperature\n",
    "  - ```map({case(k,v)=>(v,k)})``` to invert key with value and vice versa\n",
    "\n",
    "Hints & considerations:\n",
    "- Keep only temperature values <999\n",
    "- Join syntax: ```rdd1.join(rdd2)```\n",
    "  - Both RDDs should be structured as key-value RDDs with the same key: usaf + wban\n",
    "- Consider partitioning and caching to optimize the join\n",
    "  - Careful: it is not enough for the two RDDs to have the same number of partitions; they must have the same partitioner!\n",
    "- Verify the execution plan of the join in the web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ae6820-fd26-4be3-aa63-683823a6b0a3",
   "metadata": {
    "id": "e8ae6820-fd26-4be3-aa63-683823a6b0a3"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7b233",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "// Clear the cache\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbfb6fb-1c5d-450a-b0d3-42c2be226fad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:06:03.175764Z",
     "start_time": "2024-10-20T16:06:00.478094Z"
    },
    "id": "7fbfb6fb-1c5d-450a-b0d3-42c2be226fad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "// First version: caching the join\n",
    "import org.apache.spark.HashPartitioner\n",
    "val p = new HashPartitioner(8)\n",
    "\n",
    "val rddS = rddStation.\n",
    "  map(r => (r._1 + r._2, (r._3, r._4))).\n",
    "  partitionBy(p)\n",
    "val rddW = rddWeather.\n",
    "  filter(r => r._6<999).\n",
    "  map(r => (r._1 + r._2, r._6)).\n",
    "  partitionBy(p)\n",
    "\n",
    "val rddJoin = rddW.join(rddS).cache()\n",
    "\n",
    "rddJoin.\n",
    "  map(r => (r._2._2._1, r._2._1)).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d021d55e5b45dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:06:49.284408Z",
     "start_time": "2024-10-20T16:06:49.000634Z"
    }
   },
   "outputs": [],
   "source": [
    "rddJoin.\n",
    "  filter(r => r._2._2._2 == \"UK\").\n",
    "  map(r => (r._2._2._1, r._2._1)).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a26359d3cdc817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:07:16.818733Z",
     "start_time": "2024-10-20T16:07:16.302831Z"
    }
   },
   "outputs": [],
   "source": [
    "rddJoin.\n",
    "  filter(r => r._2._2._2 == \"UK\").\n",
    "  map(r => (r._2._2._1, r._2._1)).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  map({case(k,v)=>(v,k)}).\n",
    "  sortByKey(false).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802d09bf8838caa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:09:20.510836Z",
     "start_time": "2024-10-20T16:09:18.773149Z"
    }
   },
   "outputs": [],
   "source": [
    "// Second version: caching the aggregation\n",
    "import org.apache.spark.HashPartitioner\n",
    "val p = new HashPartitioner(8)\n",
    "\n",
    "val rddAgg = rddW.\n",
    "  join(rddS).\n",
    "  map(r => ((r._2._2._1,r._2._2._2), r._2._1)).\n",
    "  reduceByKey((x,y)=>{if(x._1<y._1) y else x}).\n",
    "  cache()\n",
    "\n",
    "rddAgg.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b334e4eca061b44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:09:29.350028Z",
     "start_time": "2024-10-20T16:09:29.172805Z"
    }
   },
   "outputs": [],
   "source": [
    "rddAgg.\n",
    "  filter(_._1._2==\"UK\").\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fb4461de62101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:10:02.554266Z",
     "start_time": "2024-10-20T16:10:02.297151Z"
    }
   },
   "outputs": [],
   "source": [
    "rddAgg.\n",
    "  filter(_._1._2==\"UK\").\n",
    "  map({case(k,v)=>(k,v)}).\n",
    "  sortBy(_._2,false).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882",
   "metadata": {
    "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882"
   },
   "source": [
    "## 103-4 Memory occupation\n",
    "\n",
    "Use Spark's web UI to verify the space occupied by the provided RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:12:02.380987Z",
     "start_time": "2024-10-20T16:12:02.234579Z"
    },
    "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.storage.StorageLevel._\n",
    "\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val memRdd = rddWeather.cache()\n",
    "val memSerRdd = memRdd.map(x=>x).persist(MEMORY_ONLY_SER)\n",
    "val diskRdd = memRdd.map(x=>x).persist(DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce473ab4-b676-4ae3-b7e6-a4c61f1b3c3e",
   "metadata": {
    "id": "ce473ab4-b676-4ae3-b7e6-a4c61f1b3c3e"
   },
   "source": [
    "### Solution\n",
    "\n",
    "- Collecting/Saving to file is required to trigger the (lazy) evaluation\n",
    "- memSerRdd's size will be less than half of memRdd's\n",
    "- diskRdd's size will be approximately the same as memSerRdd (output to disk is always serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404753f7-1dd6-4997-a0af-9d43f3e9c4ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T16:12:07.010670Z",
     "start_time": "2024-10-20T16:12:05.066502Z"
    },
    "id": "404753f7-1dd6-4997-a0af-9d43f3e9c4ae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "memRdd.collect()\n",
    "memSerRdd.collect()\n",
    "diskRdd.collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "302-solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
