{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "14654dff00e22200"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "81ec7f2e3271a06f"
  },
  {
   "cell_type": "markdown",
   "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee",
   "metadata": {
    "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee"
   },
   "source": [
    "# 103 Spark optimizations\n",
    "\n",
    "The goal of this lab is to understand some of the optimization mechanisms of Spark.\n",
    "\n",
    "- [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
    "- [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a037caa76dc389a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T09:18:48.618017Z",
     "start_time": "2024-11-06T09:18:25.013070Z"
    }
   },
   "source": [
    "import org.apache.spark"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-T2P39KLE:4041\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1730884719519)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f504e515fbb5fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "// DO NOT EXECUTE - this is needed just to avoid showing errors in the following cells\n",
    "val sc = spark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
   "metadata": {
    "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-06T09:18:57.480237Z",
     "start_time": "2024-11-06T09:18:55.691757Z"
    }
   },
   "source": [
    "// WEATHER structure: (usaf,wban,year,month,day,airTemperature,airTemperatureQuality)\n",
    "def parseWeather(row:String) = {\n",
    "    val usaf = row.substring(4,10)\n",
    "    val wban = row.substring(10,15)\n",
    "    val year = row.substring(15,19)\n",
    "    val month = row.substring(19,21)\n",
    "    val day = row.substring(21,23)\n",
    "    val airTemperature = row.substring(87,92)\n",
    "    val airTemperatureQuality = row.charAt(92)\n",
    "\n",
    "    (usaf,wban,year,month,day,airTemperature.toInt/10,airTemperatureQuality == '1')\n",
    "}\n",
    "\n",
    "// STATION structure: (usaf,wban,city,country,state,latitude,longitude,elevation,date_begin,date_end) \n",
    "def parseStation(row:String) = {\n",
    "    def getDouble(str:String) : Double = {\n",
    "        if (str.isEmpty)\n",
    "            return 0\n",
    "        else\n",
    "            return str.toDouble\n",
    "    }\n",
    "    val columns = row.split(\",\").map(_.replaceAll(\"\\\"\",\"\"))\n",
    "    val latitude = getDouble(columns(6))\n",
    "    val longitude = getDouble(columns(7))\n",
    "    val elevation = getDouble(columns(8))\n",
    "    (columns(0),columns(1),columns(2),columns(3),columns(4),latitude,longitude,elevation,columns(9),columns(10))  \n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseWeather: (row: String)(String, String, String, String, String, Int, Boolean)\r\n",
       "parseStation: (row: String)(String, String, String, String, String, Double, Double, Double, String, String)\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c70c02bd-4c8f-4cc2-9a13-544da7c6544d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T09:19:04.506611Z",
     "start_time": "2024-11-06T09:19:02.037259Z"
    }
   },
   "source": [
    "val rddWeather = sc.\n",
    "  textFile(\"../../../../datasets/big/weather-sample1.txt\").\n",
    "  map(x => parseWeather(x))\n",
    "val rddStation = sc.\n",
    "  textFile(\"../../../../datasets/weather-stations.csv\").\n",
    "  map(x => parseStation(x))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddWeather: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int, Boolean)] = MapPartitionsRDD[2] at map at <console>:29\r\n",
       "rddStation: org.apache.spark.rdd.RDD[(String, String, String, String, String, Double, Double, Double, String, String)] = MapPartitionsRDD[5] at map at <console>:32\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ef4b49ee-6852-4025-9e55-3950ff937680",
   "metadata": {
    "id": "ef4b49ee-6852-4025-9e55-3950ff937680"
   },
   "source": [
    "## 103-1 Simple job optimization\n",
    "\n",
    "Optimize the two jobs (avg temperature and max temperature) by avoiding the repetition of the same computations and by enforcing a partitioning criteria.\n",
    "- There are multiple methods to repartition an RDD: check the ```coalesce```, ```partitionBy```, and ```repartition``` methods on the documentation and choose the best one.\n",
    "  - To create a partitioning function, you must ```import org.apache.spark.HashPartitioner``` and then define ```val p = new HashPartitioner(n)``` where ```n``` is the number of partitions to create\n",
    "- Verify your persisted data in the web UI\n",
    "- Verify the execution plan of your RDDs with ```rdd.toDebugString``` (shell only) or on the web UI"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:24:05.666038Z",
     "start_time": "2024-10-30T08:24:05.329939Z"
    }
   },
   "cell_type": "code",
   "source": "import org.apache.spark.HashPartitioner",
   "id": "783e1984b2735d75",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
   "metadata": {
    "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-30T08:24:11.730663Z",
     "start_time": "2024-10-30T08:24:07.527185Z"
    }
   },
   "source": [
    "// Average temperature for every month\n",
    "// WEATHER structure: (usaf,wban,year,month,day,airTemperature,airTemperatureQuality)\n",
    "rddWeather.\n",
    "  filter(_._6<999).\n",
    "  map(x => (x._4, x._6)).\n",
    "  coalesce(10).\n",
    "  aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2)).\n",
    "  map({case(k,v)=>(k,Math.round(v._1*100/v._2)/100.0)}).\n",
    "  collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[(String, Double)] = Array((04,4.69), (11,4.17), (08,13.5), (09,10.35), (12,1.6), (05,9.37), (01,0.18), (06,11.67), (02,0.29), (03,1.75), (07,13.81), (10,8.33))\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "b614d5393d1a1c2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:24:19.972078Z",
     "start_time": "2024-10-30T08:24:17.251279Z"
    }
   },
   "source": [
    "// Maximum temperature for every month\n",
    "rddWeather.\n",
    "  filter(_._6<999).\n",
    "  map(x => (x._4, x._6)).\n",
    "  partitionBy(new HashPartitioner(10)).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(String, Int)] = Array((04,23), (05,34), (06,31), (07,29), (08,23), (09,30), (01,12), (10,20), (02,13), (11,14), (03,15), (12,14))\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:24:23.133514Z",
     "start_time": "2024-10-30T08:24:21.852596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "//SOLUZIONE DEL PROF\n",
    "val rddCached = rddWeather.\n",
    "  filter(_._6<999).\n",
    "  map(x => (x._4, x._6)).\n",
    "  partitionBy(new HashPartitioner(4)).  \n",
    "  cache()"
   ],
   "id": "d587f8cf4b4e0208",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddCached: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[17] at partitionBy at <console>:30\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:24:32.871143Z",
     "start_time": "2024-10-30T08:24:29.352895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rddCached.aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1), (a1,a2)=>(a1._1+a2._1,a1._2+a2._2)).\n",
    "  map({case(k,v)=>(k,Math.round(v._1*100/v._2)/100.0)}).\n",
    "  collect()"
   ],
   "id": "b235ef02342ebb3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[(String, Double)] = Array((04,4.69), (11,4.17), (08,13.5), (09,10.35), (12,1.6), (05,9.37), (01,0.18), (06,11.67), (02,0.29), (03,1.75), (07,13.81), (10,8.33))\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:24:36.263Z",
     "start_time": "2024-10-30T08:24:34.493029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rddCached.reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ],
   "id": "7c37a56561e4723e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[(String, Int)] = Array((04,23), (11,14), (08,23), (09,30), (12,14), (05,34), (01,12), (06,31), (02,13), (03,15), (07,29), (10,20))\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "377fbf30-f568-413c-9238-de139db23135",
   "metadata": {
    "id": "377fbf30-f568-413c-9238-de139db23135"
   },
   "source": [
    "## 103-2 RDD preparation\n",
    "\n",
    "Check the five possibilities to prepare the Station RDD for subsequent processing and identify the best one."
   ]
  },
  {
   "cell_type": "code",
   "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
   "metadata": {
    "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-30T08:24:48.158420Z",
     "start_time": "2024-10-30T08:24:47.216876Z"
    }
   },
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val p2 = new HashPartitioner(8)\n",
    "\n",
    "// _1 and _2 are the fields composing the key; _4 and _8 are country and elevation, respectively\n",
    "val rddS1 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p2).\n",
    "  cache().\n",
    "  map({case (k,v) => (k,(v._4,v._8))})\n",
    "// numero 1 non è adatto poichè anche se fa tutto il lavoro che ci interessa, è che la mappa viene fatta dopo il cache (quello che va in cache è il risultato di KeyBy e PartitionBy (non map). Inoltre rompiamo il criterio poichè facciamo partitionBy prima di map (NON SI FA QUESTO, NON SI MANTIENE LA PARTIZIONE DESIDERATA SE FACCIAMO MAP DOPO PARTITION!!!)\n",
    "val rddS2 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  cache().\n",
    "  partitionBy(p2)\n",
    "// numero 2 ha come problema il fatto che la cache viene fatta prima di partitionBy\n",
    "val rddS3 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  cache()\n",
    "// numero 3 ha il problema di non si mantiene criterio di partizione facendo map dopo partitionBy\n",
    "val rddS4 = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  map({case (k,v) => (k,(v._4,v._8))}).\n",
    "  partitionBy(p2).\n",
    "  cache()\n",
    "// numero 4 va bene ma deve essere ottimizzato magari mettendo inline la map\n",
    "val rddS5 = rddStation.\n",
    "  map(x => (x._1 + x._2, (x._4,x._8))).\n",
    "  partitionBy(p2).\n",
    "  cache()\n",
    "// numero 5 soluzione migliore!"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\r\n",
       "p2: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@8\r\n",
       "rddS1: org.apache.spark.rdd.RDD[(String, (String, Double))] = MapPartitionsRDD[23] at map at <console>:34\r\n",
       "rddS2: org.apache.spark.rdd.RDD[(String, (String, Double))] = ShuffledRDD[26] at partitionBy at <console>:39\r\n",
       "rddS3: org.apache.spark.rdd.RDD[(String, (String, Double))] = MapPartitionsRDD[29] at map at <console>:43\r\n",
       "rddS4: org.apache.spark.rdd.RDD[(String, (String, Double))] = ShuffledRDD[32] at partitionBy at <console>:48\r\n",
       "rddS5: org.apache.spark.rdd.RDD[(String, (String, Double))] = ShuffledRDD[34] at partitionBy at <console>:52\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed",
   "metadata": {
    "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed"
   },
   "source": [
    "## 103-3 Joining RDDs\n",
    "\n",
    "Define the join between rddWeather and rddStation and compute:\n",
    "- The maximum temperature for every city\n",
    "- The maximum temperature for every city in the UK: \n",
    "  - ```StationData.country == \"UK\"```\n",
    "- Sort the results by descending temperature\n",
    "  - ```map({case(k,v)=>(v,k)})``` to invert key with value and vice versa\n",
    "\n",
    "Hints & considerations:\n",
    "- Keep only temperature values <999\n",
    "- Join syntax: ```rdd1.join(rdd2)```\n",
    "  - Both RDDs should be structured as key-value RDDs with the same key: usaf + wban\n",
    "- Consider partitioning and caching to optimize the join\n",
    "  - Careful: it is not enough for the two RDDs to have the same number of partitions; they must have the same partitioner!\n",
    "- Verify the execution plan of the join in the web UI"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "// WEATHER structure: (usaf,wban,year,month,day,airTemperature,airTemperatureQuality)\n",
    "// STATION structure: (usaf,wban,city,country,state,latitude,longitude,elevation,date_begin,date_end) "
   ],
   "id": "2913a75b89ba3a69"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:33:34.005975Z",
     "start_time": "2024-10-30T09:33:32.190507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// STARTING CACHED WORK TO BE USED FOR THE OTHER JOBS\n",
    "val partitioner = new HashPartitioner(8)\n",
    "\n",
    "val cachedWeather = rddWeather.filter(_._6<999).keyBy(y => y._1 + y._2).cache()\n",
    "\n",
    "val rddJoinCached = rddStation.keyBy(x => x._1 + x._2)\n",
    "  .join(cachedWeather)\n",
    "  .reduceByKey((a,b) => if (a._2._6 > b._2._6) a else b)\n",
    "  .partitionBy(partitioner)\n",
    "  .cache()"
   ],
   "id": "960767805287a487",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partitioner: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@8\r\n",
       "cachedWeather: org.apache.spark.rdd.RDD[(String, (String, String, String, String, String, Int, Boolean))] = MapPartitionsRDD[79] at keyBy at <console>:37\r\n",
       "rddJoinCached: org.apache.spark.rdd.RDD[(String, ((String, String, String, String, String, Double, Double, Double, String, String), (String, String, String, String, String, Int, Boolean)))] = ShuffledRDD[85] at partitionBy at <console>:42\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:43:15.157889Z",
     "start_time": "2024-10-30T09:43:14.202305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// MAX TEMPERATURE OF EVERY CITY\n",
    "val maxTemperature = rddJoinCached.map({case (k,v) => (v._1._3, v._2._6)}).cache()\n",
    "\n",
    "maxTemperature.collect()"
   ],
   "id": "5786e38565da991f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxTemperature: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[91] at map at <console>:33\r\n",
       "res15: Array[(String, Int)] = Array((TAMPERE PIRKKALA,29), (BARRA ISLAND,21), (ISOSAARI,21), (LOSSIEMOUTH,23), (SCATSTA,20), (FOYERS,23), (BALTASOUND NO.2,18), (LAPPEENRANTA HIEKKAPAKKA,23), (UTTI,29), (VAASA,27), (LEMLAND NYHAMN,19), (FOULA,15), (INVERGORDON HARBOUR,20), (SUOMUSSALMI,29), (TULLOCH BRIDGE,26), (VAASA,25), (MARIEHAMN,23), (FOULA NO2,15), (PARAINEN UTO,20), (BUTT OF LEWIS (LH),8), (HALLI,29), (SUMBURGH,30), (AHTARI MYLLYMAKI,28), (INVERNESS,23), (SKYE/LUSA,23), (KAUHAVA,29), (STORNOWAY,20), (JOMALA,22), (JYVASKYLA,29), (KUUSAMO,29), (HANKO RUSSARO,22), (TAIN RANGE (SAWS),22), (KILMORY,21), (AVIEMORE,23), (TURKU,27), (MIKKELI,28), (AONACH MOR,17), (LAPPEENRANTA,29), (LERW...\r\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:43:17.931484Z",
     "start_time": "2024-10-30T09:43:17.268052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// MAX TEMPERATURE OF EVERY CITY IN UK\n",
    "val maxTemperatureUK = rddJoinCached.filter(_._2._1._4 == \"UK\").map({case (k,v) => (v._1._3, v._2._6)})\n",
    "\n",
    "maxTemperatureUK.collect()"
   ],
   "id": "d5eb8865147f2460",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxTemperatureUK: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[93] at map at <console>:33\r\n",
       "res16: Array[(String, Int)] = Array((BARRA ISLAND,21), (LOSSIEMOUTH,23), (SCATSTA,20), (FOYERS,23), (BALTASOUND NO.2,18), (FOULA,15), (INVERGORDON HARBOUR,20), (TULLOCH BRIDGE,26), (FOULA NO2,15), (BUTT OF LEWIS (LH),8), (SUMBURGH,30), (INVERNESS,23), (SKYE/LUSA,23), (STORNOWAY,20), (TAIN RANGE (SAWS),22), (KILMORY,21), (AVIEMORE,23), (AONACH MOR,17), (LERWICK,17), (WATERSTEIN,21), (LOCH GLASCARNOCH,22), (KIRKWALL,19), (SELLA NESS,19), (CAIRNGORM SUMMIT,14), (MUCKLE HOLM,16), (SULE SKERRY,17), (LOCHBOISDALE,21), (ALTNAHARRA NO2,24), (KINLOSS,24), (RACKWICK,19), (BENBECULA,20), (NORTH RONA ISLAND,21), (FAIR ISLE,15), (NORTH RONALDSAY ISL,6), (SOUTH UIST RANGE,34), (GLENLIVET,23), (AUL...\r\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T09:54:12.435932Z",
     "start_time": "2024-10-30T09:54:10.561388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// SORT TEMPERATURE BY DESCENDING\n",
    "// ATTENZIONE: cache() si usa solo se quel rdd creato verrà riutilizzato dopo per altri lavori grossi quali sortBy/reduceBy etc...\n",
    "// non spammare cache() a caso!!\n",
    "val sortedMaxTemperature = maxTemperature.map({case (k,v) => (v,k)}).sortByKey(false).map({case (k,v) => (v,k)})\n",
    "\n",
    "sortedMaxTemperature.collect()"
   ],
   "id": "79502af49da5a5c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sortedMaxTemperature: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[107] at sortByKey at <console>:33\r\n",
       "res20: Array[(Int, String)] = Array((34,SOUTH UIST RANGE), (31,OULU), (30,SUMBURGH), (30,SAVONLINNA), (30,KAJAANI), (30,KUOPIO), (30,JOENSUU), (29,TAMPERE PIRKKALA), (29,UTTI), (29,SUOMUSSALMI), (29,HALLI), (29,KAUHAVA), (29,JYVASKYLA), (29,KUUSAMO), (29,LAPPEENRANTA), (29,PORI), (29,KANKAANPAA NIINISALO PUOLVOIM), (29,LAHTI LAUNE), (29,VARKAUS), (28,AHTARI MYLLYMAKI), (28,MIKKELI), (28,VIITASAARI), (28,HAILUOTO ISLAND), (28,HELSINKI VANTAA), (28,SEINAJOKI), (28,NIVALA), (28,KRUUNUPYY), (28,HELSINKI MALMI), (27,VAASA), (27,TURKU), (27,JOKIOINEN), (26,TULLOCH BRIDGE), (26,SUOMUSJARVI), (25,VAASA), (25,KOTKA RANKKI), (24,ALTNAHARRA NO2), (24,KINLOSS), (23,LOSSIEMOUTH), (23,FOYERS...\r\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882",
   "metadata": {
    "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882"
   },
   "source": [
    "## 103-4 Memory occupation\n",
    "\n",
    "Use Spark's web UI to verify the space occupied by the provided RDDs."
   ]
  },
  {
   "cell_type": "code",
   "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
   "metadata": {
    "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-30T08:25:01.167480Z",
     "start_time": "2024-10-30T08:25:00.803587Z"
    }
   },
   "source": [
    "import org.apache.spark.storage.StorageLevel._\n",
    "\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val memRdd = rddWeather.cache()\n",
    "val memSerRdd = memRdd.map(x=>x).persist(MEMORY_ONLY_SER)\n",
    "val diskRdd = memRdd.map(x=>x).persist(DISK_ONLY)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.storage.StorageLevel._\r\n",
       "memRdd: rddWeather.type = MapPartitionsRDD[2] at map at <console>:29\r\n",
       "memSerRdd: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int, Boolean)] = MapPartitionsRDD[35] at map at <console>:34\r\n",
       "diskRdd: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int, Boolean)] = MapPartitionsRDD[36] at map at <console>:35\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "f4c7bc50-bb59-4e70-8955-8a44d7de774d",
   "metadata": {
    "id": "f4c7bc50-bb59-4e70-8955-8a44d7de774d"
   },
   "source": [
    "## 103-5 Evaluating different join methods\n",
    "\n",
    "Consider the following scenario:\n",
    "- We have a disposable RDD of Weather data (i.e., it is used only once): ```rddW```\n",
    "- And we have an RDD of Station data that is used many times: ```rddS```\n",
    "- Both RDDs are cached (```collect()```is called to enforce caching)\n",
    "\n",
    "We want to join the two RDDS. Which option is best?\n",
    "- Simply join the two RDDs\n",
    "- Enforce on ```rddW1``` the same partitioner of ```rddS``` (and then join)\n",
    "- Exploit broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "id": "31d77122-8bdd-4784-a86e-f42f2da06759",
   "metadata": {
    "id": "31d77122-8bdd-4784-a86e-f42f2da06759",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-06T09:19:46.977237Z",
     "start_time": "2024-11-06T09:19:34.247885Z"
    }
   },
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val p = new HashPartitioner(8)\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist()) // restituisce elenco di tutti RDD cachati, con il foreach unpersist li tolgo dalla cache\n",
    "\n",
    "val rddW = rddWeather.\n",
    "  filter(_._6<999).\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  persist() // stessa cosa di cache, solo che mi permette di specificare il livello di storage\n",
    "val rddS = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p).\n",
    "  cache()\n",
    "\n",
    "// Collect to enforce caching\n",
    "rddW.collect()\n",
    "rddS.collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\r\n",
       "p: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@8\r\n",
       "rddW: org.apache.spark.rdd.RDD[(String, (String, String, String, String, String, Int, Boolean))] = MapPartitionsRDD[7] at keyBy at <console>:33\r\n",
       "rddS: org.apache.spark.rdd.RDD[(String, (String, String, String, String, String, Double, Double, Double, String, String))] = ShuffledRDD[9] at partitionBy at <console>:37\r\n",
       "res0: Array[(String, (String, String, String, String, String, Double, Double, Double, String, String))] = Array((00701199999,(007011,99999,CWOS 07011,\"\",\"\",0.0,0.0,0.0,20120101,20121129)), (00704499999,(007044,99999,CWOS 07044,\"\",\"\",0.0,0.0,0.0,20120127,20120127)), (00840599999,(008405,99999,XM14,\"\",\"\",0.0,0.0,0.0,20120101,20120827)), (00841699999,(008416,99999,X...\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "30a6822816cd65d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T09:20:43.435749Z",
     "start_time": "2024-11-06T09:20:37.832251Z"
    }
   },
   "source": [
    "// Is it better to simply join the two RDDs..\n",
    "rddW.\n",
    "  join(rddS).\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(String, Int)] = Array((TAIN RANGE (SAWS),22), (LOSSIEMOUTH,23), (HALLI,29), (SUOMUSJARVI,26), (BALTASOUND NO.2,18), (SELLA NESS,19), (KANKAANPAA NIINISALO PUOLVOIM,29), (KIRKWALL,19), (FOULA,15), (SULE SKERRY,17), (BUTT OF LEWIS (LH),8), (KUOPIO,30), (KUMLINGE ISLAND,22), (RACKWICK,19), (STORNOWAY,20), (NORTH RONALDSAY ISL,6), (GLENLIVET,23), (WATERSTEIN,21), (KRUUNUPYY,28), (JOKIOINEN,27), (SKYE/LUSA,23), (NORTH RONA ISLAND,21), (FOULA NO2,15), (INVERGORDON HARBOUR,20), (HELSINKI VANTAA,28), (AHTARI MYLLYMAKI,28), (LERWICK,17), (HELSINKI MALMI,28), (BARRA ISLAND,21), (KUUSAMO,29), (KAUHAVA,29), (TURKU,27), (MUSTASAARI VALASSAARET,20), (CAIRNGORM SUMMIT,14), (KINLOSS,24), (VARKAUS,29), (LOCH GLASCARNOCH,22), (AULTBEA NO2,23), (LAPPEENRANTA,29), (NIVALA,28), (JYVASKYLA,29), ...\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "4e0e5f9827be45d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T09:23:40.012039Z",
     "start_time": "2024-11-06T09:23:36.914374Z"
    }
   },
   "source": [
    "// ..to enforce on rddW1 the same partitioner of rddS..\n",
    "rddW.\n",
    "  partitionBy(p).\n",
    "  join(rddS).\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[(String, Int)] = Array((TAIN RANGE (SAWS),22), (LOSSIEMOUTH,23), (HALLI,29), (SUOMUSJARVI,26), (BALTASOUND NO.2,18), (SELLA NESS,19), (KANKAANPAA NIINISALO PUOLVOIM,29), (KIRKWALL,19), (FOULA,15), (SULE SKERRY,17), (BUTT OF LEWIS (LH),8), (KUOPIO,30), (KUMLINGE ISLAND,22), (RACKWICK,19), (STORNOWAY,20), (NORTH RONALDSAY ISL,6), (GLENLIVET,23), (WATERSTEIN,21), (KRUUNUPYY,28), (JOKIOINEN,27), (SKYE/LUSA,23), (NORTH RONA ISLAND,21), (FOULA NO2,15), (INVERGORDON HARBOUR,20), (HELSINKI VANTAA,28), (AHTARI MYLLYMAKI,28), (LERWICK,17), (HELSINKI MALMI,28), (BARRA ISLAND,21), (KUUSAMO,29), (KAUHAVA,29), (TURKU,27), (MUSTASAARI VALASSAARET,20), (CAIRNGORM SUMMIT,14), (KINLOSS,24), (VARKAUS,29), (LOCH GLASCARNOCH,22), (AULTBEA NO2,23), (LAPPEENRANTA,29), (NIVALA,28), (JYVASKYLA,29), ...\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "d50b618652ac67fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T09:25:51.608615Z",
     "start_time": "2024-11-06T09:25:50.844242Z"
    }
   },
   "source": [
    "// ..or to exploit broadcast variables?\n",
    "val bRddS = sc.broadcast(rddS.map(x => (x._1, x._2._3)).collectAsMap()) //using this as a way to join in memory improves performances quite a lot!\n",
    "val rddJ = rddW.\n",
    "  map({case (k,v) => (bRddS.value.get(k),v._6)}). //inside this map the join operation is happening!\n",
    "  filter(_._1!=None)\n",
    "rddJ.\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bRddS: org.apache.spark.broadcast.Broadcast[scala.collection.Map[String,String]] = Broadcast(12)\r\n",
       "rddJ: org.apache.spark.rdd.RDD[(Option[String], Int)] = MapPartitionsRDD[23] at filter at <console>:32\r\n",
       "res3: Array[(Option[String], Int)] = Array((Some(FOYERS),23), (Some(KRUUNUPYY),28), (Some(KILMORY),21), (Some(KINLOSS),24), (Some(SUOMUSJARVI),26), (Some(TURKU),27), (Some(AULTBEA NO2),23), (Some(KUMLINGE ISLAND),22), (Some(CAIRNGORM SUMMIT),14), (Some(MARIEHAMN),23), (Some(JOMALA),22), (Some(BENBECULA),20), (Some(HANKO RUSSARO),22), (Some(SELLA NESS),19), (Some(OULU),31), (Some(AONACH MOR),17), (Some(PORI),29), (Some(JYVASKYLA),29), (Some(TULLOCH BRIDGE),26), (Some(ISOSAARI),21), (Some(STORNOWAY),20), (Some(HALLI),29), (Some(SKYE/LUSA),23), (Some(SUOMUSSALMI),29), (Some(GLENLIVET),23), (...\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "e9cc81c0-1425-4ef9-8a19-a7edca031c33",
   "metadata": {
    "id": "e9cc81c0-1425-4ef9-8a19-a7edca031c33"
   },
   "source": [
    "## 103-6 Optimizing Exercise 3\n",
    "\n",
    "Start from the result of the last job of Exercise 3; is there a more efficient way to compute the same result?\n",
    "- Try it on weather-sample10\n",
    "- Hint: consider that each station is located in only one country"
   ]
  },
  {
   "cell_type": "code",
   "id": "47748353-fb4b-432f-af79-d1136453b956",
   "metadata": {
    "id": "47748353-fb4b-432f-af79-d1136453b956",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-06T09:55:03.384318Z",
     "start_time": "2024-11-06T09:54:56.210782Z"
    }
   },
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "import org.apache.spark.storage.StorageLevel._\n",
    "val p = new HashPartitioner(8)\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val rddS = rddStation.\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p).\n",
    "  cache()\n",
    "val rddW = rddWeather.\n",
    "  filter(_._6<999).\n",
    "  keyBy(x => x._1 + x._2).\n",
    "  partitionBy(p).\n",
    "  persist(MEMORY_AND_DISK_SER)\n",
    "\n",
    "// Collect to enforce caching\n",
    "rddW.collect()\n",
    "rddS.collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.HashPartitioner\r\n",
       "import org.apache.spark.storage.StorageLevel._\r\n",
       "p: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@8\r\n",
       "rddS: org.apache.spark.rdd.RDD[(String, (String, String, String, String, String, Double, Double, Double, String, String))] = ShuffledRDD[26] at partitionBy at <console>:38\r\n",
       "rddW: org.apache.spark.rdd.RDD[(String, (String, String, String, String, String, Int, Boolean))] = ShuffledRDD[29] at partitionBy at <console>:43\r\n",
       "res4: Array[(String, (String, String, String, String, String, Double, Double, Double, String, String))] = Array((00701199999,(007011,99999,CWOS 07011,\"\",\"\",0.0,0.0,0.0,20120101,20121129)), (00704499999,(007044,99999,CWOS 07044,\"\",\"\",0.0,0.0,0.0,20120127,20120127)), (00840599999,(008405,99999,XM14,\"\",\"\",0.0,0.0,0....\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "67f448cc-efc7-4793-a3a2-4a19e0e6fc15",
   "metadata": {
    "id": "67f448cc-efc7-4793-a3a2-4a19e0e6fc15",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-06T10:02:04.812432Z",
     "start_time": "2024-11-06T10:02:01.176588Z"
    }
   },
   "source": [
    "// First version\n",
    "rddW.\n",
    "  join(rddS).\n",
    "  filter(_._2._2._4==\"UK\").\n",
    "  map({case(k,v)=>(v._2._3,v._1._6)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  map({case(k,v)=>(v,k)}).\n",
    "  sortByKey(false).\n",
    "  collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[(Int, String)] = Array((34,SOUTH UIST RANGE), (30,SUMBURGH), (26,TULLOCH BRIDGE), (24,KINLOSS), (24,ALTNAHARRA NO2), (23,LOSSIEMOUTH), (23,GLENLIVET), (23,SKYE/LUSA), (23,AULTBEA NO2), (23,INVERNESS), (23,FOYERS), (23,AVIEMORE), (22,TAIN RANGE (SAWS)), (22,LOCH GLASCARNOCH), (21,WATERSTEIN), (21,NORTH RONA ISLAND), (21,BARRA ISLAND), (21,LOCHBOISDALE), (21,KILMORY), (20,STORNOWAY), (20,INVERGORDON HARBOUR), (20,SCATSTA), (20,BENBECULA), (19,SELLA NESS), (19,KIRKWALL), (19,RACKWICK), (18,BALTASOUND NO.2), (17,SULE SKERRY), (17,LERWICK), (17,AONACH MOR), (16,MUCKLE HOLM), (15,FOULA), (15,FOULA NO2), (15,FAIR ISLE), (14,CAIRNGORM SUMMIT), (8,BUTT OF LEWIS (LH)), (6,NORTH RONALDSAY ISL))\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T11:02:56.688921Z",
     "start_time": "2024-11-06T11:02:55.049864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "// NUOVA VERSIONE\n",
    "// WEATHER structure: (usaf,wban,year,month,day,airTemperature,airTemperatureQuality)\n",
    "// STATION structure: (usaf,wban,city,country,state,latitude,longitude,elevation,date_begin,date_end)\n",
    "val filterUKStations = sc.broadcast(rddS.filter(_._2._4==\"UK\").map(pair => (pair._1, pair._2._3)).collectAsMap())\n",
    "\n",
    "rddW.\n",
    "  map({case(k,v)=>(filterUKStations.value.get(k),v._6)}). //elenco {città, temperatura}\n",
    "  filter(_._1!=None).\n",
    "  map({case (k,v) => (k.get,v)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}). //temperatura massima per ogni città\n",
    "  map({case(k,v)=>(v,k)}). //inverte chiave valore\n",
    "  sortByKey(false). //non ordina tutte le coppie chiave-valore, non è una cosa che ci interessa (decrescente)\n",
    "  collect()\n",
    "//filterUKStations.value"
   ],
   "id": "d94f8624c9668d85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterUKStations: org.apache.spark.broadcast.Broadcast[scala.collection.Map[String,String]] = Broadcast(83)\r\n",
       "res47: Array[(Int, String)] = Array((34,SOUTH UIST RANGE), (30,SUMBURGH), (26,TULLOCH BRIDGE), (24,KINLOSS), (24,ALTNAHARRA NO2), (23,LOSSIEMOUTH), (23,GLENLIVET), (23,SKYE/LUSA), (23,AULTBEA NO2), (23,INVERNESS), (23,FOYERS), (23,AVIEMORE), (22,TAIN RANGE (SAWS)), (22,LOCH GLASCARNOCH), (21,WATERSTEIN), (21,NORTH RONA ISLAND), (21,BARRA ISLAND), (21,LOCHBOISDALE), (21,KILMORY), (20,STORNOWAY), (20,INVERGORDON HARBOUR), (20,SCATSTA), (20,BENBECULA), (19,SELLA NESS), (19,KIRKWALL), (19,RACKWICK), (18,BALTASOUND NO.2), (17,SULE SKERRY), (17,LERWICK), (17,AONACH MOR), (16,MUCKLE HOLM), (15,FOULA), (15,FOULA NO2), (15,FAIR ISLE), (14,CAIRNGORM SUMMIT), (8,BUTT OF LEWIS (LH)), (6,NORT...\r\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cc934f24477544"
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "302-solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
